import React, { useState, useEffect, useRef } from 'react';
import { useApp } from '../../context/AppContext';
import { Send, Loader, Mic, Volume2, VolumeX } from 'lucide-react';
import apiService from '../../services/apiService';
import './ChatInterface.css';

const ChatInterface = () => {
  const { sessionData, addMessage, updateSession } = useApp();
  const [input, setInput] = useState('');
  const [isLoading, setIsLoading] = useState(false);
  
  // ============ STT STATE ============
  const [isRecording, setIsRecording] = useState(false);
  const [isProcessingAudio, setIsProcessingAudio] = useState(false);
  const [sttError, setSttError] = useState(null);
  const [recordingTime, setRecordingTime] = useState(0);
  
  // ============ SILENCE DETECTION STATE ============
  const [isListeningForSilence, setIsListeningForSilence] = useState(false);
  const [audioLevel, setAudioLevel] = useState(0);
  
  // ============ ğŸ“Š ENHANCED TTS STATE (OPTIMIZED) ============
  const [ttsEnabled, setTtsEnabled] = useState(true);
  const [autoVoiceSelect, setAutoVoiceSelect] = useState(true);
  const [manualVoice, setManualVoice] = useState('nova');
  const [isPlayingAudio, setIsPlayingAudio] = useState(false);
  const [speakerRole, setSpeakerRole] = useState('patient'); // 'patient' or 'mother'
  const audioRef = useRef(null);
  
  const mediaRecorderRef = useRef(null);
  const audioChunksRef = useRef([]);
  const silenceTimeoutRef = useRef(null);
  const recordingTimerRef = useRef(null);
  const messagesEndRef = useRef(null);
  const chatContainerRef = useRef(null);
  const inputRef = useRef(null);
  
  // ============ AUDIO ANALYSIS REFS ============
  const audioContextRef = useRef(null);
  const analyserRef = useRef(null);
  const animationFrameRef = useRef(null);
  const lastSoundTimeRef = useRef(0);
  const hasSpeechDetectedRef = useRef(false);

  // ============ ğŸ‘¶ CHECK IF PATIENT IS CHILD ============
  useEffect(() => {
    if (sessionData?.patientInfo) {
      const age = getPatientAge();
      const isChild = age < 12;
      setSpeakerRole(isChild ? 'mother' : 'patient');
      console.log(`ğŸ‘¥ Speaker role updated: ${isChild ? 'MOTHER' : 'PATIENT'} (age: ${age})`);
    }
  }, [sessionData?.patientInfo]);

  const scrollToBottom = () => {
    messagesEndRef.current?.scrollIntoView({ behavior: 'smooth' });
  };

  useEffect(() => {
    scrollToBottom();
  }, [sessionData?.messages]);

  useEffect(() => {
    inputRef.current?.focus();
  }, []);

  useEffect(() => {
    if (!isLoading && !isRecording) {
      inputRef.current?.focus();
    }
  }, [isLoading, isRecording, sessionData?.messages]);

  useEffect(() => {
    if (sttError) {
      const timer = setTimeout(() => {
        setSttError(null);
      }, 5000);
      return () => clearTimeout(timer);
    }
  }, [sttError]);

  useEffect(() => {
    if (isRecording) {
      recordingTimerRef.current = setInterval(() => {
        setRecordingTime(prev => prev + 1);
      }, 1000);
    } else {
      setRecordingTime(0);
      if (recordingTimerRef.current) {
        clearInterval(recordingTimerRef.current);
      }
    }
    return () => {
      if (recordingTimerRef.current) {
        clearInterval(recordingTimerRef.current);
      }
    };
  }, [isRecording]);

  const formatRecordingTime = (seconds) => {
    const mins = Math.floor(seconds / 60);
    const secs = seconds % 60;
    return `${mins}:${secs.toString().padStart(2, '0')}`;
  };

  // ============ ğŸµ ENHANCED AUDIO PLAYBACK ============
  const playAudio = (base64Audio) => {
    try {
      if (audioRef.current) {
        audioRef.current.pause();
        audioRef.current = null;
      }
      
      const audioUrl = `data:audio/mp3;base64,${base64Audio}`;
      const audio = new Audio(audioUrl);
      audioRef.current = audio;
      
      audio.onplay = () => {
        setIsPlayingAudio(true);
        console.log('ğŸ“Š TTS audio started playing');
      };
      
      audio.onended = () => {
        setIsPlayingAudio(false);
        console.log('âœ… TTS audio finished playing');
      };
      
      audio.onerror = (error) => {
        setIsPlayingAudio(false);
        console.error('âŒ TTS audio playback error:', error);
      };
      
      audio.play().catch(error => {
        console.error('âŒ Failed to play audio:', error);
        setIsPlayingAudio(false);
      });
    } catch (error) {
      console.error('âŒ Error playing audio:', error);
      setIsPlayingAudio(false);
    }
  };

  // ============ ğŸ§  BUILD CONVERSATION CONTEXT FOR AI CORRECTION ============
  const getConversationContext = () => {
    return apiService.buildConversationContext(
      sessionData?.messages || [], 
      sessionData?.caseData
    );
  };

  // ============ ğŸ‘¶ GET PATIENT AGE ============
  const getPatientAge = () => {
    if (!sessionData?.patientInfo) return 0;
    
    const ageData = sessionData.patientInfo.age;
    if (typeof ageData === 'object' && ageData.value) {
      return parseInt(ageData.value);
    } else if (typeof ageData === 'string') {
      return parseInt(ageData.match(/\d+/)?.[0] || '0');
    } else if (typeof ageData === 'number') {
      return ageData;
    }
    return 0;
  };

  // ============ ğŸ’¬ ENHANCED MESSAGE SENDING WITH OPTIMIZED TTS ============
  const handleSendMessage = async (e) => {
    e.preventDefault();
    if (!input.trim() || isLoading || !sessionData?.sessionId) return;

    const userMessage = input.trim();
    setInput('');

    addMessage({
      role: 'user',
      content: userMessage,
      timestamp: Date.now()
    });

    setIsLoading(true);

    try {
      console.log('ğŸ’¬ Sending message with optimized TTS...');
      console.log('   TTS Enabled:', ttsEnabled);
      console.log('   Auto Voice:', autoVoiceSelect);
      console.log('   Manual Voice:', manualVoice);
      console.log('   ğŸ‘¥ Speaker Role:', speakerRole);
      
      // ğŸ¯ Use auto voice selection (null) or manual override
      const selectedVoice = autoVoiceSelect ? null : manualVoice;
      
      const response = ttsEnabled 
        ? await apiService.sendMessageWithTTS(
            sessionData.sessionId, 
            userMessage, 
            true, 
            selectedVoice, // null = auto-select (mother for children <12)
            1
          )
        : await apiService.sendMessage(sessionData.sessionId, userMessage);
      
      if (response.success) {
        addMessage({
          role: 'assistant',
          content: response.data.response,
          timestamp: Date.now()
        });

        // ğŸ“Š Log voice selection and speaker role info
        if (ttsEnabled && response.data.audio) {
          console.log('ğŸ¤ TTS Voice Info (OPTIMIZED):');
          console.log('   - Voice used:', response.data.audio.voice);
          console.log('   - Auto-selected:', response.data.audio.voice_auto_selected);
          console.log('   - Speed:', response.data.audio.speed);
          console.log('   - Speaker role:', response.data.audio.speaker_role);
          console.log('   - Is child patient:', response.data.audio.is_child_patient);
          console.log('   - Thai optimized:', response.data.audio.optimized_for_thai);
          
          // Update speaker role from response
          if (response.data.audio.speaker_role) {
            setSpeakerRole(response.data.audio.speaker_role);
          }
          
          if (response.data.audio.audio_base64) {
            const speakerLabel = response.data.audio.speaker_role === 'mother' ? 'Mother' : 'Patient';
            console.log(`ğŸµ Playing ${speakerLabel}'s optimized TTS audio...`);
            playAudio(response.data.audio.audio_base64);
          }
        }

        // Update token usage
        const tokens = response.data.token_usage;
        if (tokens) {
          const currentTokenUsage = sessionData?.tokenUsage || {
            inputTokens: 0,
            outputTokens: 0,
            totalTokens: 0
          };
          
          updateSession({
            tokenUsage: {
              inputTokens: currentTokenUsage.inputTokens + tokens.input_tokens,
              outputTokens: currentTokenUsage.outputTokens + tokens.output_tokens,
              totalTokens: currentTokenUsage.totalTokens + tokens.total_tokens
            }
          });
        }
      } else {
        throw new Error(response.error || 'Failed to get response');
      }
    } catch (error) {
      console.error('Error sending message:', error);
      addMessage({
        role: 'system',
        content: `à¸‚à¹‰à¸­à¸œà¸´à¸”à¸à¸¥à¸²à¸” à¹€à¸à¸´à¸”à¸‚à¹‰à¸­à¸œà¸´à¸”à¸à¸¥à¸²à¸”: ${error.message}`,
        timestamp: Date.now()
      });
    } finally {
      setIsLoading(false);
    }
  };

  // ============ ğŸš€ AUTO-SEND MESSAGE AFTER TRANSCRIPTION ============
  const autoSendTranscribedMessage = async (transcribedText) => {
    if (!transcribedText.trim() || !sessionData?.sessionId) return;

    addMessage({
      role: 'user',
      content: transcribedText,
      timestamp: Date.now()
    });

    setIsLoading(true);

    try {
      console.log('ğŸš€ Auto-sending transcribed message with optimized TTS...');
      console.log('   ğŸ‘¥ Speaker Role:', speakerRole);
      
      // ğŸ¯ Use auto voice selection (null) or manual override
      const selectedVoice = autoVoiceSelect ? null : manualVoice;
      
      const response = ttsEnabled 
        ? await apiService.sendMessageWithTTS(
            sessionData.sessionId, 
            transcribedText, 
            true, 
            selectedVoice,
            1
          )
        : await apiService.sendMessage(sessionData.sessionId, transcribedText);
      
      if (response.success) {
        addMessage({
          role: 'assistant',
          content: response.data.response,
          timestamp: Date.now()
        });

        // ğŸ“Š Log voice selection and speaker info
        if (ttsEnabled && response.data.audio) {
          console.log('ğŸ¤ TTS Voice Info (Auto-send, OPTIMIZED):');
          console.log('   - Voice used:', response.data.audio.voice);
          console.log('   - Auto-selected:', response.data.audio.voice_auto_selected);
          console.log('   - Speaker role:', response.data.audio.speaker_role);
          console.log('   - Thai optimized:', response.data.audio.optimized_for_thai);
          
          // Update speaker role from response
          if (response.data.audio.speaker_role) {
            setSpeakerRole(response.data.audio.speaker_role);
          }
          
          if (response.data.audio.audio_base64) {
            const speakerLabel = response.data.audio.speaker_role === 'mother' ? 'Mother' : 'Patient';
            console.log(`ğŸµ Playing ${speakerLabel}'s optimized TTS audio...`);
            playAudio(response.data.audio.audio_base64);
          }
        }

        // Update token usage
        const tokens = response.data.token_usage;
        if (tokens) {
          const currentTokenUsage = sessionData?.tokenUsage || {
            inputTokens: 0,
            outputTokens: 0,
            totalTokens: 0
          };
          
          updateSession({
            tokenUsage: {
              inputTokens: currentTokenUsage.inputTokens + tokens.input_tokens,
              outputTokens: currentTokenUsage.outputTokens + tokens.output_tokens,
              totalTokens: currentTokenUsage.totalTokens + tokens.total_tokens
            }
          });
        }
      } else {
        throw new Error(response.error || 'Failed to get response');
      }
    } catch (error) {
      console.error('Error sending message:', error);
      addMessage({
        role: 'system',
        content: `à¸‚à¹‰à¸­à¸œà¸´à¸”à¸à¸¥à¸²à¸” à¹€à¸à¸´à¸”à¸‚à¹‰à¸­à¸œà¸´à¸”à¸à¸¥à¸²à¸”: ${error.message}`,
        timestamp: Date.now()
      });
    } finally {
      setIsLoading(false);
    }
  };

  // ============ ğŸ¤ AUDIO LEVEL DETECTION WITH NOISE CANCELLATION ============
  const detectAudioLevel = () => {
    if (!analyserRef.current) {
      console.log('âš ï¸ Analyser not available - cannot detect audio levels');
      return;
    }
    
    if (!mediaRecorderRef.current || mediaRecorderRef.current.state !== 'recording') {
      console.log('âš ï¸ MediaRecorder not recording - stopping detection');
      return;
    }

    const analyser = analyserRef.current;
    const bufferLength = analyser.frequencyBinCount;
    const dataArray = new Uint8Array(bufferLength);
    
    analyser.getByteFrequencyData(dataArray);
    
    let sum = 0;
    for (let i = 0; i < bufferLength; i++) {
      sum += dataArray[i];
    }
    const average = sum / bufferLength;
    
    const NOISE_THRESHOLD = 12;
    const SPEECH_THRESHOLD = 32;
    
    setAudioLevel(average);
    
    const now = Date.now();
    if (!window.lastLogTime || now - window.lastLogTime > 500) {
      console.log('ğŸ“Š Audio Level:', average.toFixed(2), 
                  '| Noise:', average > NOISE_THRESHOLD ? 'âœ…' : 'âŒ',
                  '| Speech:', average > SPEECH_THRESHOLD ? 'âœ…' : 'âŒ',
                  '| Has Speech:', hasSpeechDetectedRef.current);
      window.lastLogTime = now;
    }
    
    if (average > SPEECH_THRESHOLD) {
      lastSoundTimeRef.current = Date.now();
      
      if (!hasSpeechDetectedRef.current) {
        hasSpeechDetectedRef.current = true;
        setIsListeningForSilence(true);
        console.log('ğŸ¤ SPEECH DETECTED! Listening for silence...');
      }
    }
    
    const SILENCE_DURATION = 1250;
    const MIN_RECORDING_DURATION = 1000;
    
    const timeSinceLastSound = Date.now() - lastSoundTimeRef.current;
    const recordingDuration = Date.now() - (mediaRecorderRef.current?.startTime || Date.now());
    
    if (hasSpeechDetectedRef.current) {
      if (!window.lastTimingLog || now - window.lastTimingLog > 500) {
        console.log('â±ï¸ Time since last sound:', (timeSinceLastSound/1000).toFixed(1) + 's',
                    '| Recording duration:', (recordingDuration/1000).toFixed(1) + 's');
        window.lastTimingLog = now;
      }
    }
    
    if (hasSpeechDetectedRef.current && 
        timeSinceLastSound > SILENCE_DURATION && 
        recordingDuration > MIN_RECORDING_DURATION) {
      console.log('ğŸ”‡ SILENCE DETECTED! Auto-stopping recording...');
      console.log('   - Time since last sound:', (timeSinceLastSound/1000).toFixed(1) + 's');
      console.log('   - Total recording time:', (recordingDuration/1000).toFixed(1) + 's');
      stopRecording();
      return;
    }
    
    animationFrameRef.current = requestAnimationFrame(detectAudioLevel);
  };

  // ============ ğŸ™ï¸ START RECORDING WITH AUDIO ANALYSIS ============
  const startRecording = async () => {
    try {
      setSttError(null);
      
      if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {
        setSttError('à¹€à¸šà¸£à¸²à¸§à¹Œà¹€à¸‹à¸­à¸£à¹Œà¸‚à¸­à¸‡à¸„à¸¸à¸“à¹„à¸¡à¹ˆà¸£à¸­à¸‡à¸£à¸±à¸šà¸à¸²à¸£à¸šà¸±à¸™à¸—à¸¶à¸à¹€à¸ªà¸µà¸¢à¸‡ à¸à¸£à¸¸à¸“à¸²à¹ƒà¸Šà¹‰ Chrome, Firefox à¸«à¸£à¸·à¸­ Edge');
        return;
      }

      const stream = await navigator.mediaDevices.getUserMedia({ 
        audio: {
          channelCount: 1,
          sampleRate: 16000,
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true
        } 
      });

      try {
        audioContextRef.current = new (window.AudioContext || window.webkitAudioContext)();
        const source = audioContextRef.current.createMediaStreamSource(stream);
        analyserRef.current = audioContextRef.current.createAnalyser();
        
        analyserRef.current.fftSize = 2048;
        analyserRef.current.smoothingTimeConstant = 0.8;
        
        source.connect(analyserRef.current);
        
        console.log('âœ… Audio analyser initialized successfully');
      } catch (error) {
        console.error('âŒ Failed to initialize audio analyser:', error);
      }

      const mimeTypes = [
        'audio/webm;codecs=opus',
        'audio/webm',
        'audio/ogg;codecs=opus',
        'audio/mp4'
      ];
      
      let selectedMimeType = '';
      for (const type of mimeTypes) {
        if (MediaRecorder.isTypeSupported(type)) {
          selectedMimeType = type;
          break;
        }
      }

      if (!selectedMimeType) {
        throw new Error('à¹„à¸¡à¹ˆà¸à¸šà¸£à¸¹à¸›à¹à¸šà¸šà¸à¸²à¸£à¸šà¸±à¸™à¸—à¸¶à¸à¹€à¸ªà¸µà¸¢à¸‡à¸—à¸µà¹ˆà¸£à¸­à¸‡à¸£à¸±à¸š');
      }

      mediaRecorderRef.current = new MediaRecorder(stream, {
        mimeType: selectedMimeType,
        audioBitsPerSecond: 128000
      });

      mediaRecorderRef.current.startTime = Date.now();
      audioChunksRef.current = [];
      hasSpeechDetectedRef.current = false;
      lastSoundTimeRef.current = Date.now();

      mediaRecorderRef.current.ondataavailable = (event) => {
        if (event.data.size > 0) {
          audioChunksRef.current.push(event.data);
        }
      };

      mediaRecorderRef.current.onstop = async () => {
        stream.getTracks().forEach(track => track.stop());
        
        if (animationFrameRef.current) {
          cancelAnimationFrame(animationFrameRef.current);
        }
        if (audioContextRef.current) {
          audioContextRef.current.close();
        }
        
        setAudioLevel(0);
        setIsListeningForSilence(false);
        
        await processRecording(); 
      };

      mediaRecorderRef.current.start(100);
      setIsRecording(true);
      
      console.log('ğŸ¤ Recording started with noise cancellation enabled');

      setTimeout(() => {
        console.log('ğŸ¯ Starting audio level detection...');
        console.log('   - isRecording:', isRecording);
        console.log('   - analyserRef.current:', !!analyserRef.current);
        detectAudioLevel();
      }, 100);

      silenceTimeoutRef.current = setTimeout(() => {
        console.log('â° Auto-stopping recording after 60 seconds');
        stopRecording();
      }, 60000);
      
    } catch (error) {
      console.error('ğŸš¨ Microphone access error:', error);
      
      if (error.name === 'NotAllowedError' || error.name === 'PermissionDeniedError') {
        setSttError('à¹„à¸¡à¹ˆà¹„à¸”à¹‰à¸£à¸±à¸šà¸­à¸™à¸¸à¸à¸²à¸•à¹ƒà¸«à¹‰à¹ƒà¸Šà¹‰à¹„à¸¡à¹‚à¸„à¸£à¹‚à¸Ÿà¸™ à¸à¸£à¸¸à¸“à¸²à¸­à¸™à¸¸à¸à¸²à¸•à¸à¸²à¸£à¹€à¸‚à¹‰à¸²à¸–à¸¶à¸‡à¹„à¸¡à¹‚à¸„à¸£à¹‚à¸Ÿà¸™à¹ƒà¸™à¸à¸²à¸£à¸•à¸±à¹‰à¸‡à¸„à¹ˆà¸²à¹€à¸šà¸£à¸²à¸§à¹Œà¹€à¸‹à¸­à¸£à¹Œ');
      } else if (error.name === 'NotFoundError') {
        setSttError('à¹„à¸¡à¹ˆà¸à¸šà¹„à¸¡à¹‚à¸„à¸£à¹‚à¸Ÿà¸™ à¸à¸£à¸¸à¸“à¸²à¹€à¸Šà¸·à¹ˆà¸­à¸¡à¸•à¹ˆà¸­à¹„à¸¡à¹‚à¸„à¸£à¹‚à¸Ÿà¸™à¸à¸±à¸šà¸­à¸¸à¸›à¸à¸£à¸“à¹Œà¸‚à¸­à¸‡à¸„à¸¸à¸“');
      } else if (error.name === 'NotReadableError') {
        setSttError('à¹„à¸¡à¹ˆà¸ªà¸²à¸¡à¸²à¸£à¸–à¹€à¸‚à¹‰à¸²à¸–à¸¶à¸‡à¹„à¸¡à¹‚à¸„à¸£à¹‚à¸Ÿà¸™à¹„à¸”à¹‰ à¸­à¸²à¸ˆà¸¡à¸µà¹à¸­à¸›à¸à¸¥à¸´à¹€à¸„à¸Šà¸±à¹ˆà¸™à¸­à¸·à¹ˆà¸™à¸à¸³à¸¥à¸±à¸‡à¹ƒà¸Šà¹‰à¸‡à¸²à¸™à¸­à¸¢à¸¹à¹ˆ');
      } else {
        setSttError('à¹€à¸à¸´à¸”à¸‚à¹‰à¸­à¸œà¸´à¸”à¸à¸¥à¸²à¸”à¹ƒà¸™à¸à¸²à¸£à¹€à¸‚à¹‰à¸²à¸–à¸¶à¸‡à¹„à¸¡à¹‚à¸„à¸£à¹‚à¸Ÿà¸™: ' + error.message);
      }
    }
  };
  
  const stopRecording = () => {
    if (silenceTimeoutRef.current) {
      clearTimeout(silenceTimeoutRef.current);
      silenceTimeoutRef.current = null;
    }

    if (mediaRecorderRef.current && mediaRecorderRef.current.state !== 'inactive') {
      mediaRecorderRef.current.stop();
      setIsRecording(false);
      console.log('ğŸ›‘ Recording stopped');
    }
  };

  // ============ ğŸ§  ENHANCED AUDIO PROCESSING WITH CONTEXT-AWARE CORRECTION ============
  const processRecording = async () => {
    if (audioChunksRef.current.length === 0) {
      console.log('âŒ No audio chunks recorded');
      setSttError('à¹„à¸¡à¹ˆà¸¡à¸µà¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹€à¸ªà¸µà¸¢à¸‡à¸—à¸µà¹ˆà¸šà¸±à¸™à¸—à¸¶à¸ à¸à¸£à¸¸à¸“à¸²à¸¥à¸­à¸‡à¸­à¸µà¸à¸„à¸£à¸±à¹‰à¸‡');
      return;
    }

    setIsProcessingAudio(true);
    setSttError(null);

    try {
      const audioBlob = new Blob(audioChunksRef.current, { 
        type: mediaRecorderRef.current.mimeType 
      });

      const fileSizeKB = (audioBlob.size / 1024).toFixed(2);
      console.log(`ğŸ“Š Audio blob created: ${fileSizeKB} KB, type: ${audioBlob.type}`);

      if (audioBlob.size < 1000) {
        console.log('âŒ Audio too small:', audioBlob.size, 'bytes');
        setSttError('à¹€à¸ªà¸µà¸¢à¸‡à¸—à¸µà¹ˆà¸šà¸±à¸™à¸—à¸¶à¸à¸ªà¸±à¹‰à¸™à¹€à¸à¸´à¸™à¹„à¸› à¸à¸£à¸¸à¸“à¸²à¸à¸¹à¸”à¸™à¸²à¸™à¸‚à¸¶à¹‰à¸™à¹à¸¥à¸°à¸¥à¸­à¸‡à¸­à¸µà¸à¸„à¸£à¸±à¹‰à¸‡');
        setIsProcessingAudio(false);
        return;
      }

      console.log('ğŸ“¤ Sending audio to backend with context-aware correction...');
      
      // ğŸ§  BUILD CONVERSATION CONTEXT FOR BETTER AI CORRECTION
      const conversationContext = getConversationContext();
      console.log('ğŸ§  Context length:', conversationContext.length, 'characters');
      
      // ğŸš€ TRANSCRIBE WITH CONTEXT AND CORRECTION
      const transcription = await apiService.transcribeAudioWithContext(
        audioBlob,
        conversationContext,
        null  // Use backend default correction setting
      );

      console.log('ğŸ“¥ Transcription response:', transcription);

      if (transcription.success && transcription.data.text) {
        const finalText = transcription.data.text.trim();
        
        console.log('âœ… Transcription successful:', finalText);
        
        // ğŸ“Š LOG CORRECTION DETAILS
        if (transcription.data.correction) {
          const correction = transcription.data.correction;
          console.log('ğŸ§  AI Correction Details:');
          console.log('   - Was corrected:', correction.was_corrected);
          console.log('   - Original text:', correction.original_text);
          console.log('   - Corrected text:', correction.corrected_text);
          console.log('   - Model used:', correction.model_used);
          console.log('   - Confidence:', correction.confidence);
          console.log('   - Processing time:', correction.processing_time_ms, 'ms');
          
          if (correction.was_corrected && correction.changes && correction.changes.length > 0) {
            console.log('âœï¸ Changes made:', correction.changes.length);
            correction.changes.forEach((change, idx) => {
              console.log(`   ${idx + 1}. "${change.original}" â†’ "${change.corrected}" (${change.type})`);
            });
          }
        }
        
        // ğŸ“Š LOG PERFORMANCE METRICS
        if (transcription.data.processing_time) {
          console.log('â±ï¸ Performance Metrics:');
          console.log('   - Whisper transcription:', transcription.data.processing_time.whisper_ms, 'ms');
          console.log('   - Total processing:', transcription.data.processing_time.total_ms, 'ms');
        }
        
        if (!finalText || finalText.length < 2) {
          console.log('âŒ Transcribed text is empty or too short');
          setSttError('à¹„à¸¡à¹ˆà¸ªà¸²à¸¡à¸²à¸£à¸–à¹à¸›à¸¥à¸‡à¹€à¸ªà¸µà¸¢à¸‡à¹€à¸›à¹‡à¸™à¸‚à¹‰à¸­à¸„à¸§à¸²à¸¡à¹„à¸”à¹‰ à¸à¸£à¸¸à¸“à¸²à¸¥à¸­à¸‡à¸à¸¹à¸”à¸Šà¸±à¸”à¸‚à¸¶à¹‰à¸™');
          setIsProcessingAudio(false);
          return;
        }

        // ğŸš€ AUTO-SEND THE CORRECTED MESSAGE
        console.log('ğŸš€ Auto-sending transcribed message with optimized TTS...');
        await autoSendTranscribedMessage(finalText);
        console.log('âœ… Message auto-sent successfully!');
        
      } else {
        console.log('âŒ Transcription failed:', transcription.error);
        throw new Error(transcription.error || 'Transcription failed');
      }

    } catch (error) {
      console.error('ğŸš¨ Transcription error:', error);
      
      let userFriendlyMessage = 'à¹€à¸à¸´à¸”à¸‚à¹‰à¸­à¸œà¸´à¸”à¸à¸¥à¸²à¸”à¹ƒà¸™à¸à¸²à¸£à¹à¸›à¸¥à¸‡à¹€à¸ªà¸µà¸¢à¸‡ à¸à¸£à¸¸à¸“à¸²à¸¥à¸­à¸‡à¸­à¸µà¸à¸„à¸£à¸±à¹‰à¸‡';
      
      if (error.response?.data?.detail) {
        const errorData = error.response.data;
        
        if (errorData.detail) {
          if (typeof errorData.detail === 'object') {
            if (errorData.detail.error === 'unclear_audio') {
              userFriendlyMessage = errorData.detail.message || 'à¹„à¸¡à¹ˆà¸ªà¸²à¸¡à¸²à¸£à¸–à¹à¸›à¸¥à¸‡à¹€à¸ªà¸µà¸¢à¸‡à¹ƒà¸«à¹‰à¹€à¸›à¹‡à¸™à¸‚à¹‰à¸­à¸„à¸§à¸²à¸¡à¸—à¸µà¹ˆà¸¡à¸µà¸„à¸§à¸²à¸¡à¸«à¸¡à¸²à¸¢à¹„à¸”à¹‰';
              
              if (errorData.detail.hints) {
                console.log('ğŸ’¡ à¸„à¸³à¹à¸™à¸°à¸™à¸³:');
                errorData.detail.hints.forEach(hint => console.log('   -', hint));
              }
            } else if (errorData.detail.error === 'silent_audio') {
              userFriendlyMessage = 'à¹„à¸¡à¹ˆà¸à¸šà¹€à¸ªà¸µà¸¢à¸‡à¸à¸¹à¸”à¹ƒà¸™à¸à¸²à¸£à¸šà¸±à¸™à¸—à¸¶à¸ à¸à¸£à¸¸à¸“à¸²à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¹„à¸¡à¹‚à¸„à¸£à¹‚à¸Ÿà¸™à¹à¸¥à¸°à¸à¸¹à¸”à¹ƒà¸«à¹‰à¸Šà¸±à¸”à¹€à¸ˆà¸™';
            } else if (errorData.detail.error === 'audio_too_short') {
              userFriendlyMessage = 'à¹€à¸ªà¸µà¸¢à¸‡à¸—à¸µà¹ˆà¸šà¸±à¸™à¸—à¸¶à¸à¸ªà¸±à¹‰à¸™à¹€à¸à¸´à¸™à¹„à¸› à¸à¸£à¸¸à¸“à¸²à¸à¸¹à¸”à¸™à¸²à¸™à¸‚à¸¶à¹‰à¸™à¹à¸¥à¸°à¸¥à¸­à¸‡à¸­à¸µà¸à¸„à¸£à¸±à¹‰à¸‡';
            } else if (errorData.detail.message) {
              userFriendlyMessage = errorData.detail.message;
            }
          } else if (typeof errorData.detail === 'string') {
            userFriendlyMessage = errorData.detail;
          }
        }
      }
      
      if (error.message && !userFriendlyMessage.includes('à¹€à¸à¸´à¸”à¸‚à¹‰à¸­à¸œà¸´à¸”à¸à¸¥à¸²à¸”')) {
        if (error.message.includes('timeout') || error.message.includes('à¸«à¸¡à¸”à¹€à¸§à¸¥à¸²')) {
          userFriendlyMessage = 'à¸«à¸¡à¸”à¹€à¸§à¸¥à¸²à¹ƒà¸™à¸à¸²à¸£à¸›à¸£à¸°à¸¡à¸§à¸¥à¸œà¸¥à¹€à¸ªà¸µà¸¢à¸‡ à¸à¸£à¸¸à¸“à¸²à¸¥à¸­à¸‡à¸­à¸µà¸à¸„à¸£à¸±à¹‰à¸‡';
        } else if (error.message.includes('network') || error.message.includes('Network error')) {
          userFriendlyMessage = 'à¹€à¸à¸´à¸”à¸‚à¹‰à¸­à¸œà¸´à¸”à¸à¸¥à¸²à¸”à¹ƒà¸™à¸à¸²à¸£à¹€à¸Šà¸·à¹ˆà¸­à¸¡à¸•à¹ˆà¸­à¹€à¸„à¸£à¸·à¸­à¸‚à¹ˆà¸²à¸¢ à¸à¸£à¸¸à¸“à¸²à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¸­à¸´à¸™à¹€à¸—à¸­à¸£à¹Œà¹€à¸™à¹‡à¸•';
        }
      }
      
      setSttError(userFriendlyMessage);
    } finally {
      setIsProcessingAudio(false);
      audioChunksRef.current = [];
      console.log('ğŸ§¹ Cleanup complete');
    }
  };

  // ============ ğŸ›ï¸ TOGGLE RECORDING (SINGLE BUTTON) ============
  const toggleRecording = () => {
    if (isRecording) {
      stopRecording();
    } else {
      startRecording();
    }
  };

  const formatTime = (timestamp) => {
    const date = new Date(timestamp);
    return date.toLocaleTimeString('th-TH', { 
      hour: '2-digit', 
      minute: '2-digit' 
    });
  };

  // ============ ğŸ­ GET PATIENT INFO FOR DISPLAY ============
  const getPatientDisplayInfo = () => {
    if (sessionData?.patientInfo) {
      const patient = sessionData.patientInfo;
      const age = getPatientAge();
      const ageDisplay = typeof patient.age === 'object' ? patient.age.value : patient.age;
      return `${patient.name || 'Patient'} (${patient.sex || 'N/A'}, ${ageDisplay || 'N/A'}y)`;
    }
    return 'Virtual Patient';
  };

  // ============ ğŸ‘¥ GET SPEAKER DISPLAY LABEL ============
  const getSpeakerLabel = () => {
    if (speakerRole === 'mother') {
      return 'ğŸ‘© Mother (speaking for child)';
    }
    return 'ğŸ‘¤ Patient';
  };

  return (
    <div className="chat-interface">
      {/* âš ï¸ STT ERROR BANNER */}
      {sttError && (
        <div className="stt-error-banner">
          <div className="stt-error-content">
            <span className="stt-error-icon">âš ï¸</span>
            <span className="stt-error-text">{sttError}</span>
          </div>
          <button 
            className="stt-error-close"
            onClick={() => setSttError(null)}
            aria-label="à¸›à¸´à¸”"
          >
            âœ•
          </button>
        </div>
      )}

      {/* ğŸ“Š CHAT HEADER */}
      <div className="chat-header">
        <div className="chat-header-info">
          <div className="patient-avatar">
            {speakerRole === 'mother' ? 'ğŸ‘©' : 'ğŸ‘¤'}
          </div>
          <div>
            <h3 className="chat-title">
              {speakerRole === 'mother' ? 'Patient\'s Mother' : 'Virtual Patient'}
            </h3>
            <p className="chat-subtitle">
              {isPlayingAudio ? 'ğŸ“Š Speaking...' : 
               isListeningForSilence ? 'ğŸ‘‚ Listening...' : 
               isRecording ? 'ğŸ¤ Ready to listen' : 
               speakerRole === 'mother' ? 'ğŸ‘¶ Child patient (<12y) - Mother responds' :
               autoVoiceSelect ? 'ğŸ­ Auto Voice Mode' : 'ğŸ¤ Manual Voice'}
            </p>
          </div>
        </div>
        <div className="header-stats">
          <div className="message-count">
            ğŸ’¬ {sessionData?.messages?.length || 0} messages
          </div>
        </div>
      </div>

      {/* ğŸ’¬ CHAT MESSAGES */}
      <div className="chat-messages" ref={chatContainerRef}>
        {!sessionData?.messages?.length ? (
          <div className="empty-chat">
            <div className="empty-icon">ğŸ’¬</div>
            <h3>Start the Conversation</h3>
            <p>Click the microphone to speak or type your message.</p>
            <p className="text-sm text-muted" style={{ marginTop: '0.5rem' }}>
              ğŸ§  AI correction enabled for medical terminology
            </p>
            {speakerRole === 'mother' && (
              <p className="text-sm text-muted" style={{ marginTop: '0.5rem', color: '#8b5cf6' }}>
                ğŸ‘¶ Child patient detected - Mother will speak
              </p>
            )}
          </div>
        ) : (
          sessionData.messages.map((message, index) => (
            <div
              key={index}
              className={`message ${message.role === 'user' ? 'message-user' : 'message-assistant'} ${
                message.role === 'system' ? 'message-system' : ''
              }`}
            >
              <div className="message-avatar">
                {message.role === 'user' ? 'ğŸ§‘â€âš•ï¸' : 
                 message.role === 'system' ? 'âš ï¸' : 
                 speakerRole === 'mother' ? 'ğŸ‘©' : 'ğŸ‘¤'}
              </div>
              <div className="message-content">
                <div className="message-header">
                  <span className="message-sender">
                    {message.role === 'user' ? 'Doctor' : 
                     message.role === 'system' ? 'System' : 
                     speakerRole === 'mother' ? "Patient's Mother" : 'Patient'}
                  </span>
                  <span className="message-time">{formatTime(message.timestamp)}</span>
                </div>
                <div className="message-text">{message.content}</div>
              </div>
            </div>
          ))
        )}
        {isLoading && (
          <div className="message message-assistant">
            <div className="message-avatar">
              {speakerRole === 'mother' ? 'ğŸ‘©' : 'ğŸ‘¤'}
            </div>
            <div className="message-content">
              <div className="typing-indicator">
                <span></span>
                <span></span>
                <span></span>
              </div>
            </div>
          </div>
        )}
        <div ref={messagesEndRef} />
      </div>

      <form className="chat-input-form" onSubmit={handleSendMessage}>
        <input
          ref={inputRef}
          type="text"
          className="chat-input"
          placeholder={
            isRecording 
              ? isListeningForSilence 
                ? "ğŸ¤ à¸à¸³à¸¥à¸±à¸‡à¸Ÿà¸±à¸‡... (à¸à¸¹à¸”à¹„à¸”à¹‰à¹€à¸¥à¸¢)" 
                : "ğŸ¤ à¹€à¸£à¸´à¹ˆà¸¡à¸à¸¹à¸”à¹„à¸”à¹‰à¹€à¸¥à¸¢..."
              : isProcessingAudio 
                ? "âš™ï¸ à¸à¸³à¸¥à¸±à¸‡à¸›à¸£à¸°à¸¡à¸§à¸¥à¸œà¸¥..." 
                : "à¸à¸´à¸¡à¸à¹Œà¸‚à¹‰à¸­à¸„à¸§à¸²à¸¡à¸«à¸£à¸·à¸­à¸„à¸¥à¸´à¸à¹„à¸¡à¹‚à¸„à¸£à¹‚à¸Ÿà¸™à¹€à¸à¸·à¹ˆà¸­à¸à¸¹à¸”..."
          }
          value={input}
          onChange={(e) => setInput(e.target.value)}
          disabled={isLoading || isRecording || isProcessingAudio}
        />
        
        <button
          type="button"
          className={`btn chat-mic-btn ${ttsEnabled ? '' : 'btn-outline'}`}
          onClick={() => setTtsEnabled(!ttsEnabled)}
          disabled={isLoading || isRecording || isProcessingAudio}
          title={ttsEnabled ? 'à¸›à¸´à¸”à¹€à¸ªà¸µà¸¢à¸‡à¸•à¸­à¸šà¸à¸¥à¸±à¸š' : 'à¹€à¸›à¸´à¸”à¹€à¸ªà¸µà¸¢à¸‡à¸•à¸­à¸šà¸à¸¥à¸±à¸š'}
          style={{ 
            background: ttsEnabled ? 'linear-gradient(135deg, #8b5cf6, #7c3aed)' : undefined,
            height: '52px',
            minHeight: '52px'
          }}
        >
          {ttsEnabled ? <Volume2 size={20} /> : <VolumeX size={20} />}
        </button>
        
        <button
          type="button"
          className={`btn chat-mic-btn ${isRecording ? 'recording' : ''} ${isListeningForSilence ? 'listening' : ''}`}
          onClick={toggleRecording}
          disabled={isLoading || isProcessingAudio}
          title={isRecording ? "à¸«à¸¢à¸¸à¸”à¸šà¸±à¸™à¸—à¸¶à¸" : "à¹€à¸£à¸´à¹ˆà¸¡à¸šà¸±à¸™à¸—à¸¶à¸à¹€à¸ªà¸µà¸¢à¸‡"}
          style={{ 
            height: '52px',
            minHeight: '52px',
            boxShadow: isRecording && audioLevel > 30 
              ? `0 4px 25px rgba(239, 68, 68, ${0.3 + (audioLevel / 255) * 0.5})` 
              : undefined
          }}
        >
          {isProcessingAudio ? (
            <Loader size={20} className="spinning" />
          ) : isRecording ? (
            <>
              <Mic size={20} />
              <span style={{ marginLeft: '0.25rem', fontSize: '0.875rem' }}>
                {formatRecordingTime(recordingTime)}
              </span>
            </>
          ) : (
            <Mic size={20} />
          )}
        </button>

        <button
          type="submit"
          className="btn btn-primary chat-send-btn"
          disabled={!input.trim() || isLoading || isRecording || isProcessingAudio}
          style={{ 
            height: '52px',
            minHeight: '52px'
          }}
        >
          {isLoading ? <Loader size={20} className="spinning" /> : <Send size={20} />}
        </button>
      </form>
    </div>
  );
};

export default ChatInterface;