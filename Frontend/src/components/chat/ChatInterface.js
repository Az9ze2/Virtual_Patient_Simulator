import React, { useState, useEffect, useRef } from 'react';
import { useApp } from '../../context/AppContext';
import { Send, Loader, Mic, Volume2, VolumeX } from 'lucide-react';
import apiService from '../../services/apiService';
import './ChatInterface.css';

const ChatInterface = () => {
  const { sessionData, addMessage, updateSession } = useApp();
  const [input, setInput] = useState('');
  const [isLoading, setIsLoading] = useState(false);
  
  // ============ STT STATE ============
  const [isRecording, setIsRecording] = useState(false);
  const [isProcessingAudio, setIsProcessingAudio] = useState(false);
  const [sttError, setSttError] = useState(null);
  const [recordingTime, setRecordingTime] = useState(0);
  
  // ============ ğŸ¯ NEW: SILENCE DETECTION STATE ============
  const [isListeningForSilence, setIsListeningForSilence] = useState(false);
  const [audioLevel, setAudioLevel] = useState(0);
  
  // ============ TTS STATE ============
  const [ttsEnabled, setTtsEnabled] = useState(true);
  const [ttsVoice, setTtsVoice] = useState('nova');
  const [isPlayingAudio, setIsPlayingAudio] = useState(false);
  const audioRef = useRef(null);
  
  const mediaRecorderRef = useRef(null);
  const audioChunksRef = useRef([]);
  const silenceTimeoutRef = useRef(null);
  const recordingTimerRef = useRef(null);
  const messagesEndRef = useRef(null);
  const chatContainerRef = useRef(null);
  const inputRef = useRef(null);
  
  // ============ ğŸ¯ NEW: AUDIO ANALYSIS REFS ============
  const audioContextRef = useRef(null);
  const analyserRef = useRef(null);
  const animationFrameRef = useRef(null);
  const lastSoundTimeRef = useRef(0);
  const hasSpeechDetectedRef = useRef(false);

  const scrollToBottom = () => {
    messagesEndRef.current?.scrollIntoView({ behavior: 'smooth' });
  };

  useEffect(() => {
    scrollToBottom();
  }, [sessionData?.messages]);

  useEffect(() => {
    inputRef.current?.focus();
  }, []);

  useEffect(() => {
    if (!isLoading && !isRecording) {
      inputRef.current?.focus();
    }
  }, [isLoading, isRecording, sessionData?.messages]);

  useEffect(() => {
    if (sttError) {
      const timer = setTimeout(() => {
        setSttError(null);
      }, 5000);
      return () => clearTimeout(timer);
    }
  }, [sttError]);

  useEffect(() => {
    if (isRecording) {
      recordingTimerRef.current = setInterval(() => {
        setRecordingTime(prev => prev + 1);
      }, 1000);
    } else {
      setRecordingTime(0);
      if (recordingTimerRef.current) {
        clearInterval(recordingTimerRef.current);
      }
    }
    return () => {
      if (recordingTimerRef.current) {
        clearInterval(recordingTimerRef.current);
      }
    };
  }, [isRecording]);

  const formatRecordingTime = (seconds) => {
    const mins = Math.floor(seconds / 60);
    const secs = seconds % 60;
    return `${mins}:${secs.toString().padStart(2, '0')}`;
  };

  const playAudio = (base64Audio) => {
    try {
      if (audioRef.current) {
        audioRef.current.pause();
        audioRef.current = null;
      }
      
      const audioUrl = `data:audio/mp3;base64,${base64Audio}`;
      const audio = new Audio(audioUrl);
      audioRef.current = audio;
      
      audio.onplay = () => {
        setIsPlayingAudio(true);
        console.log('ğŸ”Š TTS audio started playing');
      };
      
      audio.onended = () => {
        setIsPlayingAudio(false);
        console.log('âœ… TTS audio finished playing');
      };
      
      audio.onerror = (error) => {
        setIsPlayingAudio(false);
        console.error('âŒ TTS audio playback error:', error);
      };
      
      audio.play().catch(error => {
        console.error('âŒ Failed to play audio:', error);
        setIsPlayingAudio(false);
      });
    } catch (error) {
      console.error('âŒ Error playing audio:', error);
      setIsPlayingAudio(false);
    }
  };

  const handleSendMessage = async (e) => {
    e.preventDefault();
    if (!input.trim() || isLoading || !sessionData?.sessionId) return;

    const userMessage = input.trim();
    setInput('');

    addMessage({
      role: 'user',
      content: userMessage,
      timestamp: Date.now()
    });

    setIsLoading(true);

    try {
      const response = ttsEnabled 
        ? await apiService.sendMessageWithTTS(sessionData.sessionId, userMessage, true, ttsVoice, 1.0)
        : await apiService.sendMessage(sessionData.sessionId, userMessage);
      
      if (response.success) {
        addMessage({
          role: 'assistant',
          content: response.data.response,
          timestamp: Date.now()
        });

        if (ttsEnabled && response.data.audio && response.data.audio.audio_base64) {
          console.log('ğŸµ Playing TTS audio response...');
          playAudio(response.data.audio.audio_base64);
        }

        const tokens = response.data.token_usage;
        if (tokens) {
          const currentTokenUsage = sessionData?.tokenUsage || {
            inputTokens: 0,
            outputTokens: 0,
            totalTokens: 0
          };
          
          updateSession({
            tokenUsage: {
              inputTokens: currentTokenUsage.inputTokens + tokens.input_tokens,
              outputTokens: currentTokenUsage.outputTokens + tokens.output_tokens,
              totalTokens: currentTokenUsage.totalTokens + tokens.total_tokens
            }
          });
        }
      } else {
        throw new Error(response.error || 'Failed to get response');
      }
    } catch (error) {
      console.error('Error sending message:', error);
      addMessage({
        role: 'system',
        content: `à¸‚à¹‰à¸­à¸œà¸´à¸”à¸à¸¥à¸²à¸” à¹€à¸à¸´à¸”à¸‚à¹‰à¸­à¸œà¸´à¸”à¸à¸¥à¸²à¸”: ${error.message}`,
        timestamp: Date.now()
      });
    } finally {
      setIsLoading(false);
    }
  };

  // ============ ğŸ¯ NEW: AUTO-SEND MESSAGE AFTER TRANSCRIPTION ============
  const autoSendTranscribedMessage = async (transcribedText) => {
    if (!transcribedText.trim() || !sessionData?.sessionId) return;

    addMessage({
      role: 'user',
      content: transcribedText,
      timestamp: Date.now()
    });

    setIsLoading(true);

    try {
      const response = ttsEnabled 
        ? await apiService.sendMessageWithTTS(sessionData.sessionId, transcribedText, true, ttsVoice, 1.0)
        : await apiService.sendMessage(sessionData.sessionId, transcribedText);
      
      if (response.success) {
        addMessage({
          role: 'assistant',
          content: response.data.response,
          timestamp: Date.now()
        });

        if (ttsEnabled && response.data.audio && response.data.audio.audio_base64) {
          console.log('ğŸµ Playing TTS audio response...');
          playAudio(response.data.audio.audio_base64);
        }

        const tokens = response.data.token_usage;
        if (tokens) {
          const currentTokenUsage = sessionData?.tokenUsage || {
            inputTokens: 0,
            outputTokens: 0,
            totalTokens: 0
          };
          
          updateSession({
            tokenUsage: {
              inputTokens: currentTokenUsage.inputTokens + tokens.input_tokens,
              outputTokens: currentTokenUsage.outputTokens + tokens.output_tokens,
              totalTokens: currentTokenUsage.totalTokens + tokens.total_tokens
            }
          });
        }
      } else {
        throw new Error(response.error || 'Failed to get response');
      }
    } catch (error) {
      console.error('Error sending message:', error);
      addMessage({
        role: 'system',
        content: `à¸‚à¹‰à¸­à¸œà¸´à¸”à¸à¸¥à¸²à¸” à¹€à¸à¸´à¸”à¸‚à¹‰à¸­à¸œà¸´à¸”à¸à¸¥à¸²à¸”: ${error.message}`,
        timestamp: Date.now()
      });
    } finally {
      setIsLoading(false);
    }
  };

  // ============ ğŸ¯ NEW: AUDIO LEVEL DETECTION WITH NOISE CANCELLATION ============
  const detectAudioLevel = () => {
    // ğŸ¯ FIX: Check refs directly, not state
    if (!analyserRef.current) {
      console.log('âš ï¸ Analyser not available - cannot detect audio levels');
      return;
    }
    
    if (!mediaRecorderRef.current || mediaRecorderRef.current.state !== 'recording') {
      console.log('âš ï¸ MediaRecorder not recording - stopping detection');
      return;
    }

    const analyser = analyserRef.current;
    const bufferLength = analyser.frequencyBinCount;
    const dataArray = new Uint8Array(bufferLength);
    
    analyser.getByteFrequencyData(dataArray);
    
    // Calculate average volume
    let sum = 0;
    for (let i = 0; i < bufferLength; i++) {
      sum += dataArray[i];
    }
    const average = sum / bufferLength;
    
    // ğŸ¯ LOWER THRESHOLDS FOR BETTER DETECTION
    const NOISE_THRESHOLD = 8;
    const SPEECH_THRESHOLD = 15;
    
    // Update audio level for visual feedback
    setAudioLevel(average);
    
    // ğŸ¯ DEBUG: Log audio levels every 500ms
    const now = Date.now();
    if (!window.lastLogTime || now - window.lastLogTime > 500) {
      console.log('ğŸ“Š Audio Level:', average.toFixed(2), 
                  '| Noise:', average > NOISE_THRESHOLD ? 'âœ…' : 'âŒ',
                  '| Speech:', average > SPEECH_THRESHOLD ? 'âœ…' : 'âŒ',
                  '| Has Speech:', hasSpeechDetectedRef.current);
      window.lastLogTime = now;
    }
    
    // Detect if there's actual speech (above speech threshold)
    if (average > SPEECH_THRESHOLD) {
      lastSoundTimeRef.current = Date.now();
      
      // Mark that we've detected speech
      if (!hasSpeechDetectedRef.current) {
        hasSpeechDetectedRef.current = true;
        setIsListeningForSilence(true);
        console.log('ğŸ¤ SPEECH DETECTED! Listening for silence...');
      }
    }
    
    // ğŸ¯ SILENCE DETECTION: Check if silent for 1.5 seconds after speech was detected
    const SILENCE_DURATION = 1500;       // Reduced from 2000 to 1500 (1.5 seconds)
    const MIN_RECORDING_DURATION = 500;  // 0.5 seconds minimum
    
    const timeSinceLastSound = Date.now() - lastSoundTimeRef.current;
    const recordingDuration = Date.now() - (mediaRecorderRef.current?.startTime || Date.now());
    
    // ğŸ¯ DEBUG: Show timing info
    if (hasSpeechDetectedRef.current) {
      if (!window.lastTimingLog || now - window.lastTimingLog > 500) {
        console.log('â±ï¸ Time since last sound:', (timeSinceLastSound/1000).toFixed(1) + 's',
                    '| Recording duration:', (recordingDuration/1000).toFixed(1) + 's');
        window.lastTimingLog = now;
      }
    }
    
    if (hasSpeechDetectedRef.current && 
        timeSinceLastSound > SILENCE_DURATION && 
        recordingDuration > MIN_RECORDING_DURATION) {
      console.log('ğŸ”‡ SILENCE DETECTED! Auto-stopping recording...');
      console.log('   - Time since last sound:', (timeSinceLastSound/1000).toFixed(1) + 's');
      console.log('   - Total recording time:', (recordingDuration/1000).toFixed(1) + 's');
      stopRecording();
      return;
    }
    
    // Continue monitoring
    animationFrameRef.current = requestAnimationFrame(detectAudioLevel);
  };

  // ============ ğŸ¯ MODIFIED: START RECORDING WITH AUDIO ANALYSIS ============
  const startRecording = async () => {
    try {
      setSttError(null);
      
      if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {
        setSttError('à¹€à¸šà¸£à¸²à¸§à¹Œà¹€à¸‹à¸­à¸£à¹Œà¸‚à¸­à¸‡à¸„à¸¸à¸“à¹„à¸¡à¹ˆà¸£à¸­à¸‡à¸£à¸±à¸šà¸à¸²à¸£à¸šà¸±à¸™à¸—à¸¶à¸à¹€à¸ªà¸µà¸¢à¸‡ à¸à¸£à¸¸à¸“à¸²à¹ƒà¸Šà¹‰ Chrome, Firefox à¸«à¸£à¸·à¸­ Edge');
        return;
      }

      // ğŸ¯ ENHANCED: Request microphone with NOISE CANCELLATION enabled
      const stream = await navigator.mediaDevices.getUserMedia({ 
        audio: {
          channelCount: 1,
          sampleRate: 16000,
          echoCancellation: true,        // âœ… Enable echo cancellation
          noiseSuppression: true,        // âœ… Enable noise suppression
          autoGainControl: true,         // âœ… Enable auto gain control
          // ğŸ¯ NEW: Advanced noise cancellation (supported in modern browsers)
          noiseSuppression: { ideal: true },
          echoCancellation: { ideal: true },
          autoGainControl: { ideal: true }
        } 
      });

      // ğŸ¯ NEW: Set up audio analysis for silence detection
      try {
        audioContextRef.current = new (window.AudioContext || window.webkitAudioContext)();
        const source = audioContextRef.current.createMediaStreamSource(stream);
        analyserRef.current = audioContextRef.current.createAnalyser();
        
        // ğŸ¯ Configure analyser for better noise detection
        analyserRef.current.fftSize = 2048;
        analyserRef.current.smoothingTimeConstant = 0.8; // Smooth out noise spikes
        
        source.connect(analyserRef.current);
        
        console.log('âœ… Audio analyser initialized successfully');
        console.log('   - FFT Size:', analyserRef.current.fftSize);
        console.log('   - Frequency Bin Count:', analyserRef.current.frequencyBinCount);
      } catch (error) {
        console.error('âŒ Failed to initialize audio analyser:', error);
        // Continue without analyser - recording will still work, just no auto-stop
      }

      const mimeTypes = [
        'audio/webm;codecs=opus',
        'audio/webm',
        'audio/ogg;codecs=opus',
        'audio/mp4'
      ];
      
      let selectedMimeType = '';
      for (const type of mimeTypes) {
        if (MediaRecorder.isTypeSupported(type)) {
          selectedMimeType = type;
          break;
        }
      }

      if (!selectedMimeType) {
        throw new Error('à¹„à¸¡à¹ˆà¸à¸šà¸£à¸¹à¸›à¹à¸šà¸šà¸à¸²à¸£à¸šà¸±à¸™à¸—à¸¶à¸à¹€à¸ªà¸µà¸¢à¸‡à¸—à¸µà¹ˆà¸£à¸­à¸‡à¸£à¸±à¸š');
      }

      mediaRecorderRef.current = new MediaRecorder(stream, {
        mimeType: selectedMimeType,
        audioBitsPerSecond: 128000
      });

      // ğŸ¯ NEW: Store start time for minimum duration check
      mediaRecorderRef.current.startTime = Date.now();

      audioChunksRef.current = [];
      hasSpeechDetectedRef.current = false;
      lastSoundTimeRef.current = Date.now();

      mediaRecorderRef.current.ondataavailable = (event) => {
        if (event.data.size > 0) {
          audioChunksRef.current.push(event.data);
        }
      };

      mediaRecorderRef.current.onstop = async () => {
        stream.getTracks().forEach(track => track.stop());
        
        // ğŸ¯ NEW: Clean up audio analysis
        if (animationFrameRef.current) {
          cancelAnimationFrame(animationFrameRef.current);
        }
        if (audioContextRef.current) {
          audioContextRef.current.close();
        }
        
        setAudioLevel(0);
        setIsListeningForSilence(false);
        
        await processRecording();
      };

      mediaRecorderRef.current.start(100);
      setIsRecording(true);
      
      console.log('ğŸ¤ Recording started with noise cancellation enabled');
      console.log('ğŸ“Š Thresholds: Noise=20, Speech=35, Silence=1.5s');
      console.log('ğŸ” Analyser available:', !!analyserRef.current);

      // ğŸ¯ NEW: Start audio level detection AFTER everything is set up
      // Use setTimeout to ensure MediaRecorder has started
      setTimeout(() => {
        console.log('ğŸ¯ Starting audio level detection...');
        console.log('   - isRecording:', isRecording);
        console.log('   - analyserRef.current:', !!analyserRef.current);
        detectAudioLevel();
      }, 100);

      // ğŸ¯ MODIFIED: Safety timeout increased to 60 seconds
      silenceTimeoutRef.current = setTimeout(() => {
        console.log('â° Auto-stopping recording after 60 seconds');
        stopRecording();
      }, 60000);
      
    } catch (error) {
      console.error('ğŸš¨ Microphone access error:', error);
      
      if (error.name === 'NotAllowedError' || error.name === 'PermissionDeniedError') {
        setSttError('à¹„à¸¡à¹ˆà¹„à¸”à¹‰à¸£à¸±à¸šà¸­à¸™à¸¸à¸à¸²à¸•à¹ƒà¸«à¹‰à¹ƒà¸Šà¹‰à¹„à¸¡à¹‚à¸„à¸£à¹‚à¸Ÿà¸™ à¸à¸£à¸¸à¸“à¸²à¸­à¸™à¸¸à¸à¸²à¸•à¸à¸²à¸£à¹€à¸‚à¹‰à¸²à¸–à¸¶à¸‡à¹„à¸¡à¹‚à¸„à¸£à¹‚à¸Ÿà¸™à¹ƒà¸™à¸à¸²à¸£à¸•à¸±à¹‰à¸‡à¸„à¹ˆà¸²à¹€à¸šà¸£à¸²à¸§à¹Œà¹€à¸‹à¸­à¸£à¹Œ');
      } else if (error.name === 'NotFoundError') {
        setSttError('à¹„à¸¡à¹ˆà¸à¸šà¹„à¸¡à¹‚à¸„à¸£à¹‚à¸Ÿà¸™ à¸à¸£à¸¸à¸“à¸²à¹€à¸Šà¸·à¹ˆà¸­à¸¡à¸•à¹ˆà¸­à¹„à¸¡à¹‚à¸„à¸£à¹‚à¸Ÿà¸™à¸à¸±à¸šà¸­à¸¸à¸›à¸à¸£à¸“à¹Œà¸‚à¸­à¸‡à¸„à¸¸à¸“');
      } else if (error.name === 'NotReadableError') {
        setSttError('à¹„à¸¡à¹ˆà¸ªà¸²à¸¡à¸²à¸£à¸–à¹€à¸‚à¹‰à¸²à¸–à¸¶à¸‡à¹„à¸¡à¹‚à¸„à¸£à¹‚à¸Ÿà¸™à¹„à¸”à¹‰ à¸­à¸²à¸ˆà¸¡à¸µà¹à¸­à¸›à¸à¸¥à¸´à¹€à¸„à¸Šà¸±à¸™à¸­à¸·à¹ˆà¸™à¸à¸³à¸¥à¸±à¸‡à¹ƒà¸Šà¹‰à¸‡à¸²à¸™à¸­à¸¢à¸¹à¹ˆ');
      } else {
        setSttError('à¹€à¸à¸´à¸”à¸‚à¹‰à¸­à¸œà¸´à¸”à¸à¸¥à¸²à¸”à¹ƒà¸™à¸à¸²à¸£à¹€à¸‚à¹‰à¸²à¸–à¸¶à¸‡à¹„à¸¡à¹‚à¸„à¸£à¹‚à¸Ÿà¸™: ' + error.message);
      }
    }
  };
  
  const stopRecording = () => {
    if (silenceTimeoutRef.current) {
      clearTimeout(silenceTimeoutRef.current);
      silenceTimeoutRef.current = null;
    }

    if (mediaRecorderRef.current && mediaRecorderRef.current.state !== 'inactive') {
      mediaRecorderRef.current.stop();
      setIsRecording(false);
      console.log('ğŸ›‘ Recording stopped');
    }
  };

  // ============ ğŸ¯ MODIFIED: AUTO-SEND AFTER SUCCESSFUL TRANSCRIPTION ============
  const processRecording = async () => {
    if (audioChunksRef.current.length === 0) {
      console.log('âŒ No audio chunks recorded');
      setSttError('à¹„à¸¡à¹ˆà¸¡à¸µà¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹€à¸ªà¸µà¸¢à¸‡à¸—à¸µà¹ˆà¸šà¸±à¸™à¸—à¸¶à¸ à¸à¸£à¸¸à¸“à¸²à¸¥à¸­à¸‡à¸­à¸µà¸à¸„à¸£à¸±à¹‰à¸‡');
      return;
    }

    setIsProcessingAudio(true);
    setSttError(null);

    try {
      const audioBlob = new Blob(audioChunksRef.current, { 
        type: mediaRecorderRef.current.mimeType 
      });

      const fileSizeKB = (audioBlob.size / 1024).toFixed(2);
      console.log(`ğŸ“Š Audio blob created: ${fileSizeKB} KB, type: ${audioBlob.type}`);

      if (audioBlob.size < 1000) {
        console.log('âŒ Audio too small:', audioBlob.size, 'bytes');
        setSttError('à¹€à¸ªà¸µà¸¢à¸‡à¸—à¸µà¹ˆà¸šà¸±à¸™à¸—à¸¶à¸à¸ªà¸±à¹‰à¸™à¹€à¸à¸´à¸™à¹„à¸› à¸à¸£à¸¸à¸“à¸²à¸à¸¹à¸”à¸™à¸²à¸™à¸‚à¸¶à¹‰à¸™à¹à¸¥à¸°à¸¥à¸­à¸‡à¸­à¸µà¸à¸„à¸£à¸±à¹‰à¸‡');
        setIsProcessingAudio(false);
        return;
      }

      console.log('ğŸ“¤ Sending audio to backend for transcription...');
      const transcription = await apiService.transcribeAudio(audioBlob);

      console.log('ğŸ“¥ Transcription response:', transcription);

      if (transcription.success && transcription.data.text) {
        const transcribedText = transcription.data.text.trim();
        
        console.log('âœ… Transcription successful:', transcribedText);
        
        if (transcribedText) {
          // ğŸ¯ AUTO-SEND: Immediately send the message
          console.log('ğŸš€ Auto-sending transcribed message...');
          await autoSendTranscribedMessage(transcribedText);
          console.log('âœ… Message auto-sent successfully!');
        } else {
          console.log('âŒ Transcribed text is empty');
          setSttError('à¹„à¸¡à¹ˆà¸ªà¸²à¸¡à¸²à¸£à¸–à¹à¸›à¸¥à¸‡à¹€à¸ªà¸µà¸¢à¸‡à¹€à¸›à¹‡à¸™à¸‚à¹‰à¸­à¸„à¸§à¸²à¸¡à¹„à¸”à¹‰ à¸à¸£à¸¸à¸“à¸²à¸¥à¸­à¸‡à¸à¸¹à¸”à¸Šà¸±à¸”à¸‚à¸¶à¹‰à¸™');
        }
      } else {
        console.log('âŒ Transcription failed:', transcription.error);
        throw new Error(transcription.error || 'Transcription failed');
      }

    } catch (error) {
      console.error('ğŸš¨ Transcription error:', error);
      
      if (error.message.includes('timeout')) {
        setSttError('à¸«à¸¡à¸”à¹€à¸§à¸¥à¸²à¹ƒà¸™à¸à¸²à¸£à¸›à¸£à¸°à¸¡à¸§à¸¥à¸œà¸¥à¹€à¸ªà¸µà¸¢à¸‡ à¸à¸£à¸¸à¸“à¸²à¸¥à¸­à¸‡à¸­à¸µà¸à¸„à¸£à¸±à¹‰à¸‡');
      } else if (error.message.includes('network')) {
        setSttError('à¹€à¸à¸´à¸”à¸‚à¹‰à¸­à¸œà¸´à¸”à¸à¸¥à¸²à¸”à¹ƒà¸™à¸à¸²à¸£à¹€à¸Šà¸·à¹ˆà¸­à¸¡à¸•à¹ˆà¸­à¹€à¸„à¸£à¸·à¸­à¸‚à¹ˆà¸²à¸¢ à¸à¸£à¸¸à¸“à¸²à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¸à¸²à¸£à¹€à¸Šà¸·à¹ˆà¸­à¸¡à¸•à¹ˆà¸­à¸­à¸´à¸™à¹€à¸—à¸­à¸£à¹Œà¹€à¸™à¹‡à¸•');
      } else if (error.message.includes('OpenAI API')) {
        setSttError('à¹€à¸à¸´à¸”à¸‚à¹‰à¸­à¸œà¸´à¸”à¸à¸¥à¸²à¸”à¸à¸±à¸š API à¸à¸£à¸¸à¸“à¸²à¸•à¸´à¸”à¸•à¹ˆà¸­à¸œà¸¹à¹‰à¸”à¸¹à¹à¸¥à¸£à¸°à¸šà¸š');
      } else {
        setSttError('à¹€à¸à¸´à¸”à¸‚à¹‰à¸­à¸œà¸´à¸”à¸à¸¥à¸²à¸”à¹ƒà¸™à¸à¸²à¸£à¹à¸›à¸¥à¸‡à¹€à¸ªà¸µà¸¢à¸‡: ' + error.message);
      }
    } finally {
      setIsProcessingAudio(false);
      audioChunksRef.current = [];
      console.log('ğŸ§¹ Cleanup complete');
    }
  };

  // ============ ğŸ¯ MODIFIED: TOGGLE RECORDING (SINGLE BUTTON) ============
  const toggleRecording = () => {
    if (isRecording) {
      stopRecording();
    } else {
      startRecording();
    }
  };

  const formatTime = (timestamp) => {
    const date = new Date(timestamp);
    return date.toLocaleTimeString('th-TH', { 
      hour: '2-digit', 
      minute: '2-digit' 
    });
  };

  return (
    <div className="chat-interface">
      {sttError && (
        <div className="stt-error-banner">
          <div className="stt-error-content">
            <span className="stt-error-icon">âš ï¸</span>
            <span className="stt-error-text">{sttError}</span>
          </div>
          <button 
            className="stt-error-close"
            onClick={() => setSttError(null)}
            aria-label="à¸›à¸´à¸”"
          >
            âœ•
          </button>
        </div>
      )}

      <div className="chat-header">
        <div className="chat-header-info">
          <div className="patient-avatar">ğŸ‘©â€âš•ï¸</div>
          <div>
            <h3 className="chat-title">Virtual Patient</h3>
            {/* ğŸ¯ NEW: Show listening status */}
            <p className="chat-subtitle">
              {isPlayingAudio ? 'ğŸ”Š Speaking...' : 
               isListeningForSilence ? 'ğŸ‘‚ Listening...' : 
               isRecording ? 'ğŸ¤ Ready to listen' : 
               'Mother Simulator'}
            </p>
          </div>
        </div>
        <div className="header-stats">
          <div className="message-count">
            ğŸ’¬ {sessionData?.messages?.length || 0} messages
          </div>
        </div>
      </div>

      <div className="chat-messages" ref={chatContainerRef}>
        {!sessionData?.messages?.length ? (
          <div className="empty-chat">
            <div className="empty-icon">ğŸ’¬</div>
            <h3>Start the Conversation</h3>
            <p>Click the microphone to speak or type your message.</p>
          </div>
        ) : (
          sessionData.messages.map((message, index) => (
            <div
              key={index}
              className={`message ${message.role === 'user' ? 'message-user' : 'message-assistant'} ${
                message.role === 'system' ? 'message-system' : ''
              }`}
            >
              <div className="message-avatar">
                {message.role === 'user' ? 'ğŸ§‘â€âš•ï¸' : message.role === 'system' ? 'âš ï¸' : 'ğŸ‘©â€âš•ï¸'}
              </div>
              <div className="message-content">
                <div className="message-header">
                  <span className="message-sender">
                    {message.role === 'user' ? 'Doctor' : message.role === 'system' ? 'System' : 'Patient'}
                  </span>
                  <span className="message-time">{formatTime(message.timestamp)}</span>
                </div>
                <div className="message-text">{message.content}</div>
              </div>
            </div>
          ))
        )}
        {isLoading && (
          <div className="message message-assistant">
            <div className="message-avatar">ğŸ‘©â€âš•ï¸</div>
            <div className="message-content">
              <div className="typing-indicator">
                <span></span>
                <span></span>
                <span></span>
              </div>
            </div>
          </div>
        )}
        <div ref={messagesEndRef} />
      </div>

      <form className="chat-input-form" onSubmit={handleSendMessage}>
        <input
          ref={inputRef}
          type="text"
          className="chat-input"
          placeholder={
            isRecording 
              ? isListeningForSilence 
                ? "ğŸ¤ à¸à¸³à¸¥à¸±à¸‡à¸Ÿà¸±à¸‡... (à¸à¸¹à¸”à¹„à¸”à¹‰à¹€à¸¥à¸¢)" 
                : "ğŸ¤ à¹€à¸£à¸´à¹ˆà¸¡à¸à¸¹à¸”à¹„à¸”à¹‰à¹€à¸¥à¸¢..."
              : isProcessingAudio 
                ? "âš™ï¸ à¸à¸³à¸¥à¸±à¸‡à¸›à¸£à¸°à¸¡à¸§à¸¥à¸œà¸¥..." 
                : "à¸à¸´à¸¡à¸à¹Œà¸‚à¹‰à¸­à¸„à¸§à¸²à¸¡à¸«à¸£à¸·à¸­à¸„à¸¥à¸´à¸à¹„à¸¡à¹‚à¸„à¸£à¹‚à¸Ÿà¸™à¹€à¸à¸·à¹ˆà¸­à¸à¸¹à¸”..."
          }
          value={input}
          onChange={(e) => setInput(e.target.value)}
          disabled={isLoading || isRecording || isProcessingAudio}
        />
        
        <button
          type="button"
          className={`btn chat-mic-btn ${ttsEnabled ? '' : 'btn-outline'}`}
          onClick={() => setTtsEnabled(!ttsEnabled)}
          disabled={isLoading || isRecording || isProcessingAudio}
          title={ttsEnabled ? 'à¸›à¸´à¸”à¹€à¸ªà¸µà¸¢à¸‡à¸•à¸­à¸šà¸à¸¥à¸±à¸š' : 'à¹€à¸›à¸´à¸”à¹€à¸ªà¸µà¸¢à¸‡à¸•à¸­à¸šà¸à¸¥à¸±à¸š'}
          style={{ 
            background: ttsEnabled ? 'linear-gradient(135deg, #8b5cf6, #7c3aed)' : undefined,
            height: '52px',
            minHeight: '52px'
          }}
        >
          {ttsEnabled ? <Volume2 size={20} /> : <VolumeX size={20} />}
        </button>
        
        {/* ğŸ¯ MODIFIED: Single microphone button with visual feedback */}
        <button
          type="button"
          className={`btn chat-mic-btn ${isRecording ? 'recording' : ''} ${isListeningForSilence ? 'listening' : ''}`}
          onClick={toggleRecording}
          disabled={isLoading || isProcessingAudio}
          title={isRecording ? "à¸«à¸¢à¸¸à¸”à¸šà¸±à¸™à¸—à¸¶à¸" : "à¹€à¸£à¸´à¹ˆà¸¡à¸šà¸±à¸™à¸—à¸¶à¸à¹€à¸ªà¸µà¸¢à¸‡"}
          style={{ 
            height: '52px',
            minHeight: '52px',
            // ğŸ¯ NEW: Visual feedback based on audio level
            boxShadow: isRecording && audioLevel > 30 
              ? `0 4px 25px rgba(239, 68, 68, ${0.3 + (audioLevel / 255) * 0.5})` 
              : undefined
          }}
        >
          {isProcessingAudio ? (
            <Loader size={20} className="spinning" />
          ) : isRecording ? (
            <>
              <Mic size={20} />
              <span style={{ marginLeft: '0.25rem', fontSize: '0.875rem' }}>
                {formatRecordingTime(recordingTime)}
              </span>
            </>
          ) : (
            <Mic size={20} />
          )}
        </button>

        <button
          type="submit"
          className="btn btn-primary chat-send-btn"
          disabled={!input.trim() || isLoading || isRecording || isProcessingAudio}
          style={{ 
            height: '52px',
            minHeight: '52px'
          }}
        >
          {isLoading ? <Loader size={20} className="spinning" /> : <Send size={20} />}
        </button>
      </form>
    </div>
  );
};

export default ChatInterface;