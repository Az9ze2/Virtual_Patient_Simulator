"""
Enhanced Speech-to-Text (STT) Routes with Medical Terminology Correction
Uses OpenAI Whisper API + GPT-4 mini for Thai medical conversations
"""

from fastapi import APIRouter, UploadFile, File, HTTPException, Body
from fastapi.responses import JSONResponse
import openai
import os
from typing import Dict, Any, Optional
import tempfile
import logging
import json
import re

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Create router
router = APIRouter()

# Initialize OpenAI client
openai.api_key = os.getenv("OPENAI_API_KEY")

if not openai.api_key:
    logger.warning("‚ö†Ô∏è  OPENAI_API_KEY not found! STT features will not work.")
else:
    logger.info("‚úÖ OpenAI API key loaded successfully")


# ============================================
# üéØ CONFIGURATION PARAMETERS (Adjustable)
# ============================================

class STTConfig:
    """
    Centralized configuration for STT correction system
    Adjust these parameters for optimal performance
    """
    
    # Feature toggles
    ENABLE_CORRECTION = True  # üîß Master switch for correction feature
    
    # Correction model settings
    CORRECTION_MODEL = "gpt-4o-mini"  # üîß Options: "gpt-4o-mini", "gpt-4o", "gpt-3.5-turbo"
    CORRECTION_TEMPERATURE = 0.1  # üîß Lower = more conservative (0.0-1.0)
    CORRECTION_MAX_TOKENS = 200  # üîß Max tokens for correction response
    
    # Performance settings
    CORRECTION_TIMEOUT = 3.0  # üîß Max seconds for correction API call (affects speed)
    USE_STREAMING = False  # üîß Enable streaming for faster perceived response
    
    # Context settings
    USE_CONVERSATION_CONTEXT = True  # üîß Use chat history for better corrections
    MAX_CONTEXT_MESSAGES = 3  # üîß Number of previous messages to include
    
    # Validation settings
    MIN_TEXT_LENGTH = 2  # üîß Minimum characters to process
    VALIDATE_THAI_ONLY = True  # üîß Ensure output is Thai language
    
    # Medical terms database (expandable)
    COMMON_MEDICAL_TERMS = [
        "‡∏≠‡∏≤‡∏Å‡∏≤‡∏£", "‡∏õ‡∏ß‡∏î", "‡∏õ‡∏ß‡∏î‡∏´‡∏±‡∏ß", "‡∏õ‡∏ß‡∏î‡∏ó‡πâ‡∏≠‡∏á", "‡∏õ‡∏ß‡∏î‡∏´‡∏•‡∏±‡∏á",
        "‡πÑ‡∏Ç‡πâ", "‡∏Ñ‡∏•‡∏∑‡πà‡∏ô‡πÑ‡∏™‡πâ", "‡∏≠‡∏≤‡πÄ‡∏à‡∏µ‡∏¢‡∏ô", "‡∏ó‡πâ‡∏≠‡∏á‡πÄ‡∏™‡∏µ‡∏¢", "‡∏ó‡πâ‡∏≠‡∏á‡∏ú‡∏π‡∏Å",
        "‡πÅ‡∏û‡πâ", "‡πÅ‡∏û‡πâ‡∏¢‡∏≤", "‡πÅ‡∏û‡πâ‡∏≠‡∏≤‡∏´‡∏≤‡∏£", "‡πÅ‡∏û‡πâ‡∏≠‡∏≤‡∏Å‡∏≤‡∏®",
        "‡πÄ‡∏ö‡∏≤‡∏´‡∏ß‡∏≤‡∏ô", "‡∏Ñ‡∏ß‡∏≤‡∏°‡∏î‡∏±‡∏ô", "‡∏´‡∏≠‡∏ö‡∏´‡∏∑‡∏î", "‡∏†‡∏π‡∏°‡∏¥‡πÅ‡∏û‡πâ",
        "‡πÇ‡∏£‡∏Ñ", "‡πÇ‡∏£‡∏Ñ‡∏õ‡∏£‡∏∞‡∏à‡∏≥‡∏ï‡∏±‡∏ß", "‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥", "‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏£‡∏±‡∏ß",
        "‡∏ï‡∏£‡∏ß‡∏à", "‡∏ß‡∏¥‡∏ô‡∏¥‡∏à‡∏â‡∏±‡∏¢", "‡∏£‡∏±‡∏Å‡∏©‡∏≤", "‡∏¢‡∏≤", "‡∏Å‡∏¥‡∏ô‡∏¢‡∏≤",
        "‡∏ú‡∏∑‡πà‡∏ô", "‡∏Ñ‡∏±‡∏ô", "‡∏ö‡∏ß‡∏°", "‡πÄ‡∏à‡πá‡∏ö", "‡∏à‡∏∏‡∏Å‡πÄ‡∏™‡∏µ‡∏¢‡∏î",
        "‡πÄ‡∏´‡∏ô‡∏∑‡πà‡∏≠‡∏¢", "‡∏´‡∏≠‡∏ö", "‡πÄ‡∏à‡πá‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡∏≠‡∏Å", "‡πÉ‡∏à‡∏™‡∏±‡πà‡∏ô"
    ]

# Global config instance
stt_config = STTConfig()


# ============================================
# üß† CORRECTION SYSTEM
# ============================================

def build_correction_prompt(
    transcribed_text: str,
    conversation_context: Optional[str] = None
) -> str:
    """
    Build optimized prompt for medical terminology correction
    
    Args:
        transcribed_text: Raw Whisper transcription
        conversation_context: Recent chat history for context
    
    Returns:
        Correction prompt for GPT
    """
    
    prompt = f"""‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô‡∏ú‡∏π‡πâ‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏û‡∏ó‡∏¢‡πå‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢

‡∏á‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì: ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏ñ‡∏≠‡∏î‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏°‡∏≤‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡πÅ‡∏û‡∏ó‡∏¢‡πå‡∏Å‡∏±‡∏ö‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢

‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç: "{transcribed_text}"
"""

    if conversation_context and stt_config.USE_CONVERSATION_CONTEXT:
        prompt += f"\n\n‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤:\n{conversation_context}\n"
    
    prompt += """
‡∏Å‡∏é‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç:
1. ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏™‡∏∞‡∏Å‡∏î‡∏ú‡∏¥‡∏î‡∏´‡∏£‡∏∑‡∏≠‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢
2. ‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢‡πÅ‡∏•‡∏∞‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡∏±‡πâ‡∏á‡πÉ‡∏à‡πÄ‡∏î‡∏¥‡∏°‡∏Ç‡∏≠‡∏á‡∏ú‡∏π‡πâ‡∏û‡∏π‡∏î
3. ‡πÉ‡∏ä‡πâ‡∏Ñ‡∏≥‡∏®‡∏±‡∏û‡∏ó‡πå‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏û‡∏ó‡∏¢‡πå‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
4. ‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô ‡∏´‡πâ‡∏≤‡∏°‡∏°‡∏µ‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©‡∏´‡∏£‡∏∑‡∏≠‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô
5. ‡∏ñ‡πâ‡∏≤‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß ‡πÉ‡∏´‡πâ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°
6. ‡∏´‡πâ‡∏≤‡∏°‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏´‡∏£‡∏∑‡∏≠‡∏•‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç
7. ‡πÉ‡∏ä‡πâ‡∏ô‡πâ‡∏≥‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡πÅ‡∏•‡∏∞‡∏•‡∏µ‡∏•‡∏≤‡∏Ç‡∏≠‡∏á‡∏ú‡∏π‡πâ‡∏û‡∏π‡∏î‡πÄ‡∏î‡∏¥‡∏°
8. ‡∏≠‡∏¥‡∏á‡∏≠‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏®‡∏±‡∏û‡∏ó‡πå‡∏ó‡∏µ‡πà‡πÅ‡∏Å‡πâ‡∏à‡∏≤‡∏Å‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢‡∏£‡∏π‡∏õ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ

‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á:
- "‡∏õ‡∏ß‡∏î‡∏´‡∏±‡∏ß‡∏°‡∏≤‡∏Å" ‚Üí "‡∏õ‡∏ß‡∏î‡∏´‡∏±‡∏ß‡∏°‡∏≤‡∏Å" (‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß)
- "‡∏õ‡∏ß‡∏ó‡∏´‡∏±‡∏ß" ‚Üí "‡∏õ‡∏ß‡∏î‡∏´‡∏±‡∏ß"
- "‡∏ú‡∏°‡∏°‡∏µ‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡∏õ‡∏ß‡∏î‡∏ó‡πâ‡∏≠‡∏á‡∏Ñ‡∏£‡∏±‡∏ö" ‚Üí "‡∏ú‡∏°‡∏°‡∏µ‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡∏õ‡∏ß‡∏î‡∏ó‡πâ‡∏≠‡∏á‡∏Ñ‡∏£‡∏±‡∏ö"
- "‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏Ç‡∏±‡∏°‡∏≤‡∏Å‡∏Ñ‡∏£‡∏±‡∏ö" ‚Üí "‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏Ç‡πâ‡∏°‡∏≤‡∏Å‡∏Ñ‡∏£‡∏±‡∏ö"
- "‡πÄ‡∏Ñ‡∏¢‡∏™‡∏µ‡πà‡∏´‡∏£‡∏∑‡∏≠‡∏≠‡∏∂‡∏ö‡πâ‡∏≤‡∏á‡∏°‡∏±‡πâ‡∏¢" ‚Üí "‡πÄ‡∏Ñ‡∏¢‡∏â‡∏µ‡πà‡∏´‡∏£‡∏∑‡∏≠‡∏≠‡∏∂‡∏ö‡πâ‡∏≤‡∏á‡∏°‡∏±‡πâ‡∏¢"

‡∏ï‡∏≠‡∏ö‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÅ‡∏•‡πâ‡∏ß ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢:"""

    return prompt


async def correct_transcription(
    text: str,
    conversation_context: Optional[str] = None
) -> Dict[str, Any]:
    """
    Correct medical terminology and misspellings using GPT
    
    Args:
        text: Transcribed text from Whisper
        conversation_context: Recent conversation for context
    
    Returns:
        Dict with corrected text and metadata
    """
    
    # Check if correction is enabled
    if not stt_config.ENABLE_CORRECTION:
        logger.info("üîß Correction disabled - returning original text")
        return {
            "corrected_text": text,
            "original_text": text,
            "was_corrected": False,
            "correction_applied": False
        }
    
    # Skip correction for very short text
    if len(text.strip()) < stt_config.MIN_TEXT_LENGTH:
        logger.info(f"‚è≠Ô∏è Text too short ({len(text)} chars) - skipping correction")
        return {
            "corrected_text": text,
            "original_text": text,
            "was_corrected": False,
            "reason": "text_too_short"
        }
    
    try:
        logger.info(f"üß† Starting correction for: '{text[:50]}...'")
        start_time = __import__('time').time()
        
        # Build prompt
        prompt = build_correction_prompt(text, conversation_context)
        
        # Call GPT for correction with timeout
        response = openai.chat.completions.create(
            model=stt_config.CORRECTION_MODEL,
            messages=[
                {
                    "role": "system",
                    "content": "‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô‡∏ú‡∏π‡πâ‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡∏î‡πâ‡∏≤‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡πÅ‡∏•‡∏∞‡∏Ñ‡∏≥‡∏®‡∏±‡∏û‡∏ó‡πå‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏û‡∏ó‡∏¢‡πå"
                },
                {
                    "role": "user",
                    "content": prompt
                }
            ],
            temperature=stt_config.CORRECTION_TEMPERATURE,
            max_tokens=stt_config.CORRECTION_MAX_TOKENS,
            timeout=stt_config.CORRECTION_TIMEOUT
        )
        
        corrected_text = response.choices[0].message.content.strip()
        
        # Remove quotes if GPT added them
        corrected_text = corrected_text.strip('"\'')
        
        # Validate Thai language only
        if stt_config.VALIDATE_THAI_ONLY:
            # Allow Thai characters, spaces, and common punctuation
            if not re.match(r'^[\u0E00-\u0E7F‡πÖ‡∏Ø\s.,!?()]+$', corrected_text):
                logger.warning("‚ö†Ô∏è Correction contains non-Thai characters - using original")
                corrected_text = text
        
        elapsed_time = __import__('time').time() - start_time
        was_corrected = corrected_text != text
        
        logger.info(f"‚úÖ Correction complete in {elapsed_time:.2f}s")
        logger.info(f"   Original:  '{text}'")
        logger.info(f"   Corrected: '{corrected_text}'")
        logger.info(f"   Changed: {'YES' if was_corrected else 'NO'}")
        
        return {
            "corrected_text": corrected_text,
            "original_text": text,
            "was_corrected": was_corrected,
            "correction_applied": True,
            "model_used": stt_config.CORRECTION_MODEL,
            "processing_time_ms": round(elapsed_time * 1000, 2)
        }
        
    except openai.APITimeoutError as e:
        logger.warning(f"‚è±Ô∏è Correction timeout - using original text: {str(e)}")
        return {
            "corrected_text": text,
            "original_text": text,
            "was_corrected": False,
            "error": "timeout",
            "fallback": True
        }
    
    except Exception as e:
        logger.error(f"‚ùå Correction error: {str(e)}")
        # Fallback to original text on error
        return {
            "corrected_text": text,
            "original_text": text,
            "was_corrected": False,
            "error": str(e),
            "fallback": True
        }


# ============================================
# üì° API ENDPOINTS
# ============================================

@router.post("/transcribe")
async def transcribe_audio(
    audio: UploadFile = File(...),
    conversation_context: Optional[str] = Body(None),
    enable_correction: Optional[bool] = Body(None)
) -> Dict[str, Any]:
    """
    Transcribe audio to text with optional correction
    
    Args:
        audio: Audio file (webm, mp4, wav, ogg)
        conversation_context: Recent chat history for better corrections
        enable_correction: Override global correction setting
    
    Returns:
        Transcription with optional correction results
    """
    
    # Check if API key is available
    if not openai.api_key:
        raise HTTPException(
            status_code=500,
            detail={
                "error": "api_not_configured",
                "message": "‡∏£‡∏∞‡∏ö‡∏ö‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏¥‡∏î‡∏ï‡πà‡∏≠‡∏ú‡∏π‡πâ‡∏î‡∏π‡πÅ‡∏•‡∏£‡∏∞‡∏ö‡∏ö",
                "technical_detail": "OpenAI API key not configured"
            }
        )
    
    try:
        # ============ STEP 1: VALIDATE FILE ============
        allowed_types = [
            'audio/webm', 
            'audio/mp4', 
            'audio/mpeg', 
            'audio/wav', 
            'audio/ogg',
            'audio/webm;codecs=opus',
            'audio/x-m4a',
            'audio/mp4a-latm'
        ]
        
        if audio.content_type not in allowed_types:
            raise HTTPException(
                status_code=400,
                detail={
                    "error": "unsupported_format",
                    "message": f"‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡πÑ‡∏°‡πà‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö: {audio.content_type}",
                    "supported_formats": ["webm", "mp4", "mpeg", "wav", "ogg"]
                }
            )
        
        # Read and validate file size
        content = await audio.read()
        file_size_mb = len(content) / (1024 * 1024)
        
        logger.info(f"üìä Received audio: {audio.filename}, size: {file_size_mb:.2f} MB")
        
        if file_size_mb > 25:
            raise HTTPException(
                status_code=400,
                detail={
                    "error": "file_too_large",
                    "message": f"‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡πÉ‡∏´‡∏ç‡πà‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ ({file_size_mb:.2f} MB)",
                    "max_size_mb": 25
                }
            )
        
        if file_size_mb < 0.001:  # Less than 1KB
            raise HTTPException(
                status_code=400,
                detail={
                    "error": "file_too_small",
                    "message": "‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡πÄ‡∏•‡πá‡∏Å‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡πÉ‡∏´‡∏°‡πà"
                }
            )
        
        # ============ STEP 2: WHISPER TRANSCRIPTION (Fast) ============
        extension_map = {
            'audio/webm': '.webm',
            'audio/webm;codecs=opus': '.webm',
            'audio/mp4': '.mp4',
            'audio/x-m4a': '.m4a',
            'audio/mpeg': '.mp3',
            'audio/wav': '.wav',
            'audio/ogg': '.ogg'
        }
        extension = extension_map.get(audio.content_type, '.webm')
        
        with tempfile.NamedTemporaryFile(delete=False, suffix=extension) as temp_file:
            temp_file.write(content)
            temp_file_path = temp_file.name
        
        try:
            logger.info(f"üé§ Starting Whisper transcription...")
            whisper_start = __import__('time').time()
            
            with open(temp_file_path, 'rb') as audio_file:
                transcript = openai.audio.transcriptions.create(
                    model="whisper-1",
                    file=audio_file,
                    language="th",
                    response_format="json",
                    temperature=0.0,
                    prompt="‡∏ô‡∏µ‡πà‡∏Ñ‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏û‡∏ó‡∏¢‡πå‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡∏Ç‡∏≠‡∏á‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢ ‡πÉ‡∏ä‡πâ‡∏Ñ‡∏≥‡∏®‡∏±‡∏û‡∏ó‡πå‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏û‡∏ó‡∏¢‡πå‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢"
                )
            
            whisper_time = __import__('time').time() - whisper_start
            transcribed_text = transcript.text.strip()
            
            logger.info(f"‚úÖ Whisper done in {whisper_time:.2f}s: '{transcribed_text[:100]}...'")
            
            # ============ STEP 4: CHECK FOR EMPTY AUDIO ============
            silent_patterns = [
                "‡πÇ‡∏õ‡∏£‡∏î‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏°‡∏ï‡∏≠‡∏ô‡∏ï‡πà‡∏≠‡πÑ‡∏õ",
                ""  # Empty string
            ]
            if not transcribed_text or (len(transcribed_text.strip()) == 0 or transcribed_text in silent_patterns):
                logger.warning(f"‚ö†Ô∏è Empty transcription - no audio detected")
                raise HTTPException(
                    status_code=400,
                    detail={
                        "error": "silent_audio",
                        "message": "‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏û‡∏π‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏û‡∏π‡∏î‡πÉ‡∏´‡πâ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô"
                    }
                )
            
            # ============ STEP 4: CORRECTION (Optional, Fast) ============
            correction_result = None
            final_text = transcribed_text
            
            # Use parameter override or global setting
            should_correct = enable_correction if enable_correction is not None else stt_config.ENABLE_CORRECTION
            
            if should_correct:
                logger.info("üß† Applying correction...")
                correction_result = await correct_transcription(
                    transcribed_text,
                    conversation_context
                )
                final_text = correction_result["corrected_text"]
            else:
                logger.info("‚è≠Ô∏è Correction disabled - using raw transcription")
            
            # ============ STEP 5: PREPARE RESPONSE ============
            total_time = __import__('time').time() - whisper_start
            
            response_data = {
                "text": final_text,
                "original_transcription": transcribed_text,
                "language": "th",
                "whisper_model": "whisper-1",
                "file_size_mb": round(file_size_mb, 2),
                "processing_time": {
                    "whisper_ms": round(whisper_time * 1000, 2),
                    "total_ms": round(total_time * 1000, 2)
                }
            }
            
            # Add correction metadata if applied
            if correction_result:
                response_data["correction"] = correction_result
            
            logger.info(f"‚úÖ STT pipeline complete in {total_time:.2f}s")
            logger.info(f"   Final text: '{final_text}'")
            
            return {
                "success": True,
                "data": response_data,
                "message": "Transcription completed successfully"
            }
            
        finally:
            # Clean up temporary file
            if os.path.exists(temp_file_path):
                os.remove(temp_file_path)
    
    except HTTPException:
        raise
    
    except Exception as e:
        logger.error(f"üö® Transcription Error: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail={
                "error": "transcription_failed",
                "message": "‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏™‡∏µ‡∏¢‡∏á ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏•‡∏≠‡∏á‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á",
                "technical_detail": str(e)
            }
        )


@router.get("/status")
async def stt_status() -> Dict[str, Any]:
    """Check STT service status and configuration"""
    api_key_configured = bool(openai.api_key)
    
    return {
        "success": api_key_configured,
        "data": {
            "service": "OpenAI Whisper + GPT Correction",
            "whisper_model": "whisper-1",
            "correction_model": stt_config.CORRECTION_MODEL,
            "language": "th",
            "api_key_configured": api_key_configured,
            "max_file_size_mb": 25,
            "supported_formats": ["webm", "mp4", "mpeg", "wav", "ogg"],
            "features": {
                "whisper_transcription": True,
                "correction_enabled": stt_config.ENABLE_CORRECTION,
                "context_aware": stt_config.USE_CONVERSATION_CONTEXT,
                "thai_validation": stt_config.VALIDATE_THAI_ONLY
            },
            "configuration": {
                "correction_model": stt_config.CORRECTION_MODEL,
                "correction_temperature": stt_config.CORRECTION_TEMPERATURE,
                "correction_timeout": stt_config.CORRECTION_TIMEOUT,
                "max_context_messages": stt_config.MAX_CONTEXT_MESSAGES
            }
        },
        "message": "STT service is ready" if api_key_configured else "OpenAI API key not configured"
    }


@router.get("/health")
async def stt_health() -> Dict[str, Any]:
    """Health check endpoint for STT service"""
    api_key_configured = bool(openai.api_key)
    
    return {
        "status": "healthy" if api_key_configured else "degraded",
        "api_available": api_key_configured,
        "service": "STT",
        "components": {
            "whisper": api_key_configured,
            "correction": stt_config.ENABLE_CORRECTION and api_key_configured,
            "validation": True
        }
    }


@router.post("/config")
async def update_stt_config(config_update: Dict[str, Any]) -> Dict[str, Any]:
    """
    Update STT configuration at runtime
    
    Example request body:
    {
        "enable_correction": true,
        "correction_model": "gpt-4o-mini",
        "correction_temperature": 0.1,
        "use_conversation_context": true
    }
    """
    
    updated_fields = []
    
    if "enable_correction" in config_update:
        stt_config.ENABLE_CORRECTION = bool(config_update["enable_correction"])
        updated_fields.append("enable_correction")
    
    if "correction_model" in config_update:
        stt_config.CORRECTION_MODEL = config_update["correction_model"]
        updated_fields.append("correction_model")
    
    if "correction_temperature" in config_update:
        stt_config.CORRECTION_TEMPERATURE = float(config_update["correction_temperature"])
        updated_fields.append("correction_temperature")
    
    if "correction_timeout" in config_update:
        stt_config.CORRECTION_TIMEOUT = float(config_update["correction_timeout"])
        updated_fields.append("correction_timeout")
    
    if "use_conversation_context" in config_update:
        stt_config.USE_CONVERSATION_CONTEXT = bool(config_update["use_conversation_context"])
        updated_fields.append("use_conversation_context")
    
    if "max_context_messages" in config_update:
        stt_config.MAX_CONTEXT_MESSAGES = int(config_update["max_context_messages"])
        updated_fields.append("max_context_messages")
    
    logger.info(f"üîß Configuration updated: {updated_fields}")
    
    return {
        "success": True,
        "message": f"Updated {len(updated_fields)} configuration fields",
        "updated_fields": updated_fields,
        "current_config": {
            "enable_correction": stt_config.ENABLE_CORRECTION,
            "correction_model": stt_config.CORRECTION_MODEL,
            "correction_temperature": stt_config.CORRECTION_TEMPERATURE,
            "correction_timeout": stt_config.CORRECTION_TIMEOUT,
            "use_conversation_context": stt_config.USE_CONVERSATION_CONTEXT,
            "max_context_messages": stt_config.MAX_CONTEXT_MESSAGES
        }
    }