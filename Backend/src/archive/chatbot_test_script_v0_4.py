#!/usr/bin/env python3
"""
Thai Medical OSCE Chatbot with Memory Management
Supports both GPT-4.1-mini (tunable) and GPT-5 (deterministic).
"""

import json
import time
import sys
import os
from typing import Dict, Any, Tuple, List
from dotenv import load_dotenv

from openai import OpenAI
from langchain_core.messages import AIMessage, HumanMessage, SystemMessage


class SimpleChatbotTester:
    """Chatbot tester with token tracking + memory-saving options"""

    def __init__(self, memory_mode: str = "summarize", model_choice: str = "gpt-4.1-mini"):
        """
        Initialize chatbot tester
        memory_mode options:
            - "none": keep all history
            - "truncate": drop oldest messages
            - "summarize": compress old turns into a summary

        model_choice options:
            - "gpt-4.1-mini" (supports tunable params)
            - "gpt-5" (deterministic, limited params)
        """
        load_dotenv()
        self.api_key = os.getenv('OPENAI_API_KEY')
        if not self.api_key:
            print("‚ùå OPENAI_API_KEY not found in .env file")
            sys.exit(1)

        self.client = OpenAI(api_key=self.api_key)

        self.model_choice = model_choice

        # ‚úÖ Handle model-specific generation params
        if self.model_choice == "gpt-4.1-mini":
            self.generation_params = {
                "model": "gpt-4.1-mini",
                "temperature": 0.7,
                "top_p": 0.9,
                "frequency_penalty": 0.3,
                "presence_penalty": 0.5,
                "max_completion_tokens": 600,
                "stop": ["üßë‚Äç‚öïÔ∏è"]
            }
        elif self.model_choice == "gpt-5":
            self.generation_params = {
                "model": "gpt-5",
                "max_completion_tokens": 600
                # ‚ö†Ô∏è GPT-5 does not support temperature/top_p/penalties/stop
            }
        else:
            print(f"‚ùå Unsupported model: {self.model_choice}")
            sys.exit(1)

        self.message_history: List[Any] = []
        self.total_tokens = 0
        self.input_tokens = 0
        self.output_tokens = 0

        self.memory_mode = memory_mode
        self.max_turns_before_memory = 10

        print(f"‚úì Initialized model = {self.model_choice}, memory mode = {self.memory_mode}")

    def load_case_data(self, json_file_path: str) -> Dict[str, Any]:
        """Load case data from JSON"""
        try:
            with open(json_file_path, 'r', encoding='utf-8') as f:
                case_data = json.load(f)
            print(f"üìã Loaded case: {case_data['case_metadata']['case_title']}")
            return case_data
        except Exception as e:
            print(f"‚úó Error loading case: {e}")
            sys.exit(1)

    def setup_conversation(self, case_data: Dict[str, Any]) -> str:
        """Prepare system prompt and initial context"""
        mother_profile = case_data['simulation_view']['mother_profile']
        simulation_instructions = case_data['simulation_view']['simulation_instructions']
        examiner_view = case_data.get('examiner_view', {})
        patient_bg = examiner_view.get('patient_background', {})
        child_name = patient_bg.get('child_name', '‡∏î.‡∏ä.‡∏¢‡∏¥‡∏ô‡∏î‡∏µ ‡∏õ‡∏£‡∏µ‡∏î‡∏≤')
        child_age_data = patient_bg.get('child_age', patient_bg.get('child_age_days_or_months', '5 ‡∏ß‡∏±‡∏ô'))
        if isinstance(child_age_data, dict):
            child_age = f"{child_age_data.get('value', 5)} {child_age_data.get('unit', '‡∏ß‡∏±‡∏ô')}"
        else:
            child_age = child_age_data
        questions_to_ask = simulation_instructions.get('questions_to_ask_examiner', [])
        question_list = ""
        for q in questions_to_ask:
            question_text = q.get('examiner_questions', q.get('questions', ''))
            question_list += f"‚Üí {question_text}\n"

        sample_dialogues = simulation_instructions.get('sample_dialogue', [])
        dialogue_examples = ""
        if sample_dialogues:
            dialogue_examples = "\n# ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤\n"
            for dialogue_group in sample_dialogues[:3]:  # Use first 3 dialogue groups
                if isinstance(dialogue_group.get('topic'), list):
                    # Latest format with conversation flow
                    dialogue_examples += f"## {dialogue_group.get('description', '‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤')}\n"
                    for exchange in dialogue_group['topic'][:2]:  # Show 2 exchanges per topic
                        if exchange.get('role') == 'examiner':
                            dialogue_examples += f"‡∏´‡∏°‡∏≠: {exchange.get('text', '')}\n"
                        elif exchange.get('role') == 'mother':
                            dialogue_examples += f"‡∏Ñ‡∏∏‡∏ì: {exchange.get('text', '')}\n"
                    dialogue_examples += "\n"

        system_prompt = f"""
        # ‡∏Å‡∏è‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö
        - ‡∏ï‡∏≠‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô ‡πÉ‡∏ä‡πâ‡∏†‡∏≤‡∏©‡∏≤‡∏ó‡∏µ‡πà‡∏á‡πà‡∏≤‡∏¢ ‡πÑ‡∏°‡πà‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô
        - ‡πÉ‡∏´‡πâ‡∏ï‡∏≠‡∏ö‡∏™‡∏±‡πâ‡∏ô‡πÜ ‡∏Å‡∏£‡∏∞‡∏ä‡∏±‡∏ö ‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏ô 2-3 ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ
        - ‡πÑ‡∏°‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡∏û‡∏π‡∏î‡∏ñ‡∏∂‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏™‡∏î‡∏á‡∏ó‡πà‡∏≤‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏¢‡∏†‡∏≤‡∏û
        - ‡∏≠‡∏¢‡πà‡∏≤‡πÉ‡∏´‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÉ‡∏ô‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß 
        - ‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏û‡∏ó‡∏¢‡πå‡πÄ‡∏î‡πá‡∏î‡∏Ç‡∏≤‡∏î

        # ‡∏ö‡∏ó‡∏ö‡∏≤‡∏ó‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì
        ‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡πÅ‡∏°‡πà‡∏Ç‡∏≠‡∏á‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢‡∏ó‡∏µ‡πà‡∏°‡∏≤‡∏û‡∏ö‡πÅ‡∏û‡∏ó‡∏¢‡πå‡πÉ‡∏ô‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ ‡πÇ‡∏î‡∏¢‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡∏±‡∏á‡∏ô‡∏µ‡πâ:
        {mother_profile.get('name')} ({mother_profile.get('age')} ‡∏õ‡∏µ, {mother_profile.get('occupation')})
        ‡πÅ‡∏°‡πà‡∏Ç‡∏≠‡∏á {child_name} ‡∏≠‡∏≤‡∏¢‡∏∏ {child_age}

        # ‡∏°‡∏≤‡πÇ‡∏£‡∏á‡∏û‡∏¢‡∏≤‡∏ö‡∏≤‡∏•‡πÄ‡∏û‡∏£‡∏≤‡∏∞
        {simulation_instructions.get('scenario')}

        # ‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞‡∏ô‡∏¥‡∏™‡∏±‡∏¢
        - {mother_profile.get('emotional_state')}

        {dialogue_examples}

        # ‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏≤‡∏Å‡∏£‡∏π‡πâ
        - ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ñ‡∏≤‡∏°‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏ô‡∏µ‡πâ‡πÑ‡∏î‡πâ‡∏ï‡πà‡∏≠‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏Ñ‡∏∏‡∏ì‡∏´‡∏°‡∏≠‡πÄ‡∏õ‡∏¥‡∏î‡πÇ‡∏≠‡∏Å‡∏≤‡∏™‡πÉ‡∏´‡πâ‡∏ñ‡∏≤‡∏°
        - ‡πÇ‡∏î‡∏¢‡πÉ‡∏´‡πâ‡∏ñ‡∏≤‡∏°‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÑ‡∏î‡πâ‡∏ó‡∏µ‡∏•‡∏∞ 1 ‡∏Ç‡πâ‡∏≠‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
        ‡∏•‡∏¥‡∏™‡∏ï‡πå‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏≤‡∏Å‡∏£‡∏π‡πâ:
        {question_list}
        """

        self.message_history = [SystemMessage(content=system_prompt)]
        return system_prompt

    def summarize_history(self):
        """Summarize old messages into a shorter form"""
        if len(self.message_history) <= 6:
            return

        old_msgs = self.message_history[1:-5]
        convo_text = "\n".join(
            [f"{'üßë‚Äç‚öïÔ∏è' if isinstance(m, HumanMessage) else 'üë©‚Äç‚öïÔ∏è'}: {m.content}" for m in old_msgs]
        )

        summarization_prompt = [
            {"role": "system", "content": "‡∏™‡∏£‡∏∏‡∏õ‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡πÉ‡∏´‡πâ‡∏™‡∏±‡πâ‡∏ô ‡∏Å‡∏£‡∏∞‡∏ä‡∏±‡∏ö ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏Å‡πá‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏£‡∏á‡∏à‡∏≥‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏ô‡πÑ‡∏Ç‡πâ"},
            {"role": "user", "content": convo_text}
        ]

        summary_resp = self.client.chat.completions.create(
            messages=summarization_prompt,
            model="gpt-4.1-mini",  # always use tunable model for summarization
            temperature=0.3,
            max_completion_tokens=200
        )

        summary_text = summary_resp.choices[0].message.content.strip()
        self.message_history = [self.message_history[0]] + [
            SystemMessage(content=f"[‡∏™‡∏£‡∏∏‡∏õ‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤]: {summary_text}")
        ] + self.message_history[-5:]

        # ‚úÖ Print live summary update
        print("\nüßæ Memory updated with new summary:\n")
        print(f"[SUMMARY] {summary_text}\n")

    def manage_memory(self):
        """Decide whether to truncate or summarize"""
        if self.memory_mode == "truncate":
            self.message_history = [self.message_history[0]] + self.message_history[-10:]
        elif self.memory_mode == "summarize":
            self.summarize_history()

    def manage_memory(self):
        """Decide whether to truncate or summarize"""
        if self.memory_mode == "truncate":
            self.message_history = [self.message_history[0]] + self.message_history[-10:]
        elif self.memory_mode == "summarize":
            self.summarize_history()

    def chat_turn(self, student_message: str) -> Tuple[str, float]:
        """Handle one conversation turn"""
        start_time = time.time()
        try:
            self.message_history.append(HumanMessage(content=student_message))

            if len(self.message_history) > self.max_turns_before_memory:
                self.manage_memory()

            messages_payload = []
            for m in self.message_history:
                if isinstance(m, SystemMessage):
                    messages_payload.append({"role": "system", "content": m.content})
                elif isinstance(m, HumanMessage):
                    messages_payload.append({"role": "user", "content": m.content})
                elif isinstance(m, AIMessage):
                    messages_payload.append({"role": "assistant", "content": m.content})

            response = self.client.chat.completions.create(
                messages=messages_payload,
                **self.generation_params
            )

            usage = response.usage
            self.input_tokens += usage.prompt_tokens
            self.output_tokens += usage.completion_tokens
            self.total_tokens += usage.total_tokens

            patient_response = response.choices[0].message.content
            self.message_history.append(AIMessage(content=patient_response))

            return patient_response, time.time() - start_time

        except Exception as e:
            return f"‡∏Ç‡∏≠‡πÇ‡∏ó‡∏©‡∏Ñ‡πà‡∏∞ ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {str(e)}", 0.0

    def show_session_summary(self):
        """Show token usage and stats"""
        print("\nüìä ‡∏™‡∏£‡∏∏‡∏õ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô")
        print(f"üì• Input Tokens: {self.input_tokens}")
        print(f"üì§ Output Tokens: {self.output_tokens}")
        print(f"üìä Total Tokens: {self.total_tokens}")

    def show_context_summary(self):
        """Print out the current conversation memory (including summary if applied)"""
        print("\nüìù Current conversation context:\n")
        for m in self.message_history:
            role = "SYSTEM" if isinstance(m, SystemMessage) else (
                "STUDENT" if isinstance(m, HumanMessage) else "MOTHER"
            )
            print(f"[{role}] {m.content[:200]}{'...' if len(m.content) > 200 else ''}")


def main():
    case_file = "output.json"
    if not os.path.exists(case_file):
        print(f"‚ùå Missing case file: {case_file}")
        sys.exit(1)

    # Choose model here:
    # "gpt-4.1-mini" ‚Üí tunable, interactive
    # "gpt-5"        ‚Üí deterministic, strict
    bot = SimpleChatbotTester(memory_mode="summarize", model_choice="gpt-4.1-mini")

    case_data = bot.load_case_data(case_file)
    bot.setup_conversation(case_data)

    print("\nüí¨ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤ (quit = ‡∏à‡∏ö)\n")
    while True:
        student = input("üßë‚Äç‚öïÔ∏è ‡∏ô‡∏±‡∏Å‡∏®‡∏∂‡∏Å‡∏©‡∏≤: ").strip()
        if student.lower() in ["quit", "exit", "q"]:
            break
        reply, t = bot.chat_turn(student)
        print(f"üë©‚Äç‚öïÔ∏è ‡∏°‡∏≤‡∏£‡∏î‡∏≤: {reply} (‚è± {t:.2f}s)")

    bot.show_session_summary()
    bot.show_context_summary()   # ‚úÖ final memory state


if __name__ == "__main__":
    main()
