 #!/usr/bin/env python3
"""
Thai Medical OSCE Chatbot with Memory Management
Supports both GPT-4.1-mini (tunable) and GPT-5 (deterministic).
"""

import argparse
import json
import time
import sys
import os
import re
from typing import Dict, Any, Tuple, List
from dotenv import load_dotenv

from openai import OpenAI
from langchain_core.messages import AIMessage, HumanMessage, SystemMessage


class SimpleChatbotTester:
    """Chatbot tester with token tracking + memory-saving options"""

    def __init__(self, memory_mode: str = "summarize", model_choice: str = "gpt-4.1-mini", exam_mode: bool = False):
        """
        Initialize chatbot tester
        memory_mode options:
            - "none": keep all history
            - "truncate": drop oldest messages
            - "summarize": compress old turns into a summary

        model_choice options:
            - "gpt-4.1-mini" (supports tunable params)
            - "gpt-5" (deterministic, limited params)
        """
        load_dotenv()
        self.api_key = os.getenv('OPENAI_API_KEY')
        if not self.api_key:
            print("‚ùå OPENAI_API_KEY not found in .env file")
            sys.exit(1)

        self.client = OpenAI(api_key=self.api_key)

        self.model_choice = model_choice

        # ‚úÖ Handle model-specific generation params
        if self.model_choice == "gpt-4.1-mini":
            self.generation_params = {
                "model": "gpt-4.1-mini",
                "temperature": 0.7,
                "top_p": 0.85,
                "frequency_penalty": 0.6,
                "presence_penalty": 0.9,
                "max_completion_tokens": 600,
                "stop": ["üßë‚Äç‚öïÔ∏è"]
            }
        elif self.model_choice == "gpt-5":
            self.generation_params = {
                "model": "gpt-5",
                "max_completion_tokens": 600
                # ‚ö†Ô∏è GPT-5 does not support temperature/top_p/penalties/stop
            }
        else:
            print(f"‚ùå Unsupported model: {self.model_choice}")
            sys.exit(1)

        # ‚úÖ Apply exam mode seed
        if exam_mode:
            self.generation_params["seed"] = 1234  # fixed for reproducibility
            print("üéì Exam mode enabled: fixed seed for standardized responses")
        else:
            print("üß™ Practice mode enabled: natural variation (no seed)")

        self.message_history: List[Any] = []
        self.total_tokens = 0
        self.input_tokens = 0
        self.output_tokens = 0

        self.memory_mode = memory_mode
        self.max_turns_before_memory = 10

        print(f"‚úì Initialized model = {self.model_choice}, memory mode = {self.memory_mode}")

    def load_case_data(self, json_file_path: str) -> Dict[str, Any]:
        """Load case data from JSON"""
        try:
            with open(json_file_path, 'r', encoding='utf-8') as f:
                case_data = json.load(f)
            print(f"üìã Loaded case: {case_data['case_metadata']['case_title']}")
            return case_data
        except Exception as e:
            print(f"‚úó Error loading case: {e}")
            sys.exit(1)

    def setup_conversation(self, case_data: Dict[str, Any]) -> str:
        """Prepare system prompt and initial context"""
        simulator_profile = case_data['simulation_view']['simulator_profile']
        simulation_instructions = case_data['simulation_view']['simulation_instructions']
        examiner_view = case_data.get('examiner_view', {})
        patient_bg = examiner_view.get('patient_background', {})
        simulator_name = simulator_profile.get('name')
        simulator_age_data = simulator_profile.get('age')
        simulator_sex = patient_bg.get('sex')
        simulator_occupation = simulator_profile.get('occupation')
        simulator_illness_history = examiner_view.get('patient_illness_history', {})
        simulator_illness_timeline = examiner_view.get('symptoms_timeline', {})
        if isinstance(simulator_age_data, dict):
            simulator_age = f"{simulator_age_data.get('value')} {simulator_age_data.get('unit')}"
        else:
            simulator_age = simulator_age_data

        # ‚úÖ Handle fallback_question format and add tracking
        fallback_data = simulation_instructions.get("fallback_question")
        
        # Handle cases where fallback_question is null or missing
        if fallback_data is None:
            self.questions_to_ask = []
            self.question_limit_type = None
            print("‚ÑπÔ∏è No fallback questions found in this case")
        else:
            # Extract question limit type
            self.question_limit_type = fallback_data.get("question_limit_type", "single")  # Default to single
            
            questions_data = fallback_data.get("questions", [])
            self.questions_to_ask = [
                {
                    "simulator_ask": q.get("text", ""),
                    "asked": False
                }
                for q in questions_data if q.get("text", "").strip()  # Only include non-empty questions
            ]
            
            if not self.questions_to_ask:
                print("‚ÑπÔ∏è No valid questions found in fallback_question")
            else:
                print(f"‚ÑπÔ∏è Question limit type: {self.question_limit_type}")

        sample_dialogues = simulation_instructions.get('sample_dialogue', [])
        dialogue_examples = ""
        if sample_dialogues:
            dialogue_examples = "\n# ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤\n"
            for dialogue_group in sample_dialogues[:3]:  # Use first 3 dialogue groups
                if isinstance(dialogue_group.get('topic'), list):
                    # Latest format with conversation flow
                    dialogue_examples += f"## {dialogue_group.get('description', '‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤')}\n"
                    for exchange in dialogue_group['topic'][:2]:  # Show 2 exchanges per topic
                        if exchange.get('role') == 'examiner':
                            dialogue_examples += f"‡∏´‡∏°‡∏≠: {exchange.get('text', '')}\n"
                        elif exchange.get('role') == 'mother':
                            dialogue_examples += f"‡∏Ñ‡∏∏‡∏ì: {exchange.get('text', '')}\n"
                    dialogue_examples += "\n"

        system_prompt = f"""
        # ‡∏Å‡∏è‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö
        - ‡∏´‡∏≤‡∏Å‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏î‡πá‡∏Å ‡πÉ‡∏´‡πâ‡∏ï‡∏≠‡∏ö‡πÉ‡∏ô‡∏ê‡∏≤‡∏ô‡∏∞‡∏°‡∏≤‡∏£‡∏î‡∏≤‡∏Ç‡∏≠‡∏á‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢ ‡∏´‡∏£‡∏∑‡∏≠ ‡∏ú‡∏π‡πâ‡∏õ‡∏Å‡∏Ñ‡∏£‡∏≠‡∏á‡∏ó‡∏µ‡πà‡∏°‡∏≤‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏±‡∏ö‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢
        - ‡∏ñ‡πâ‡∏≤‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢‡πÄ‡∏õ‡πá‡∏ô‡∏ú‡∏π‡πâ‡πÉ‡∏´‡∏ç‡πà ‡πÉ‡∏´‡πâ‡∏ï‡∏≠‡∏ö‡πÉ‡∏ô‡∏ê‡∏≤‡∏ô‡∏∞‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢‡πÄ‡∏≠‡∏á
        - ‡∏ï‡∏≠‡∏ö‡∏ï‡∏≤‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ‡∏°‡∏≤‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô ‡∏≠‡∏¢‡πà‡∏≤‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡∏°‡πà
        - ‡∏ï‡∏≠‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô ‡πÉ‡∏ä‡πâ‡∏†‡∏≤‡∏©‡∏≤‡∏ó‡∏µ‡πà‡∏á‡πà‡∏≤‡∏¢ ‡πÑ‡∏°‡πà‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô
        - ‡πÉ‡∏´‡πâ‡∏ï‡∏≠‡∏ö‡∏™‡∏±‡πâ‡∏ô‡πÜ ‡∏Å‡∏£‡∏∞‡∏ä‡∏±‡∏ö ‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏ô 2-3 ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ
        - ‡∏≠‡∏¢‡πà‡∏≤‡πÉ‡∏´‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÉ‡∏ô‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß ‡πÅ‡∏•‡∏∞‡∏´‡∏≤‡∏Å‡∏ú‡∏π‡πâ‡∏™‡∏≠‡∏ö‡∏ñ‡∏≤‡∏°‡πÑ‡∏°‡πà‡∏ñ‡∏≤‡∏° ‡∏≠‡∏¢‡πà‡∏≤‡πÄ‡∏•‡πà‡∏≤‡πÉ‡∏´‡πâ‡∏ü‡∏±‡∏á
        - ‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏û‡∏ó‡∏¢‡πå‡∏´‡∏£‡∏∑‡∏≠‡πÅ‡∏™‡∏î‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏û‡∏ó‡∏¢‡πå
        - ‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ó‡∏µ‡πà‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏° ‡πÉ‡∏´‡πâ‡πÅ‡∏ô‡πà‡πÉ‡∏à‡∏ß‡πà‡∏≤‡∏ï‡∏≠‡∏ö‡πÉ‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡∏û‡∏π‡∏î ‡πÅ‡∏•‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏ò‡∏£‡∏£‡∏°‡∏ä‡∏≤‡∏ï‡∏¥‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î

        # ‡∏ö‡∏ó‡∏ö‡∏≤‡∏ó‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì
        ‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢‡∏ó‡∏µ‡πà‡∏°‡∏≤‡∏û‡∏ö‡πÅ‡∏û‡∏ó‡∏¢‡πå‡πÉ‡∏ô‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ ‡πÇ‡∏î‡∏¢‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡∏±‡∏á‡∏ô‡∏µ‡πâ:
        {simulator_name}, ‡∏≠‡∏≤‡∏¢‡∏∏ ({simulator_age} ‡∏õ‡∏µ, ‡πÄ‡∏û‡∏® {simulator_sex}, ‡∏≠‡∏≤‡∏ä‡∏µ‡∏û {simulator_occupation})

        # ‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞‡∏ô‡∏¥‡∏™‡∏±‡∏¢
        - {simulator_profile.get('emotional_state')}

        # ‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°
        - {simulation_instructions.get('behavior')}

        # ‡∏°‡∏≤‡πÇ‡∏£‡∏á‡∏û‡∏¢‡∏≤‡∏ö‡∏≤‡∏•‡πÄ‡∏û‡∏£‡∏≤‡∏∞
        {simulation_instructions.get('scenario')}

        # ‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡∏õ‡πà‡∏ß‡∏¢
        - {simulator_illness_history}

        # ‡∏•‡∏≥‡∏î‡∏±‡∏ö‡πÄ‡∏ß‡∏•‡∏≤‡∏Ç‡∏≠‡∏á‡∏≠‡∏≤‡∏Å‡∏≤‡∏£ ‡πÅ‡∏•‡∏∞‡∏£‡∏∞‡∏¢‡∏∞‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô
        - {simulator_illness_timeline}

        # ‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏≤‡∏Å‡∏£‡∏π‡πâ (‡∏Å‡∏é‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç) 
        ‡∏ñ‡πâ‡∏≤‡∏ú‡∏π‡πâ‡πÄ‡∏Ç‡πâ‡∏≤‡∏™‡∏≠‡∏ö‡∏ñ‡∏≤‡∏°‡∏ß‡πà‡∏≤ "‡∏°‡∏µ‡∏≠‡∏∞‡πÑ‡∏£‡∏ñ‡∏≤‡∏°‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà" ‡∏´‡∏£‡∏∑‡∏≠ "‡∏°‡∏µ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏≠‡∏∞‡πÑ‡∏£‡∏≠‡∏µ‡∏Å‡∏°‡∏±‡πâ‡∏¢":
        {self._build_question_limit_rule()}
        ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏° (‡∏ñ‡∏≤‡∏°‡πÄ‡∏â‡∏û‡∏≤‡∏∞ asked=False):
        {self._build_question_list()}
        
        **‡∏Å‡∏é‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç**: ‡∏ñ‡πâ‡∏≤‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÅ‡∏™‡∏î‡∏á "[‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏° - ‡∏ú‡∏π‡πâ‡πÄ‡∏Ç‡πâ‡∏≤‡∏™‡∏≠‡∏ö‡πÑ‡∏î‡πâ‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°‡∏ó‡∏∏‡∏Å‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡πÅ‡∏•‡πâ‡∏ß]" 
        ‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏≠‡∏ö‡∏ß‡πà‡∏≤ "‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°‡∏Ñ‡πà‡∏∞" ‡∏´‡∏£‡∏∑‡∏≠ "‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏û‡∏∑‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°‡∏Ñ‡∏£‡∏±‡∏ö" ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô ‡∏Ç‡∏∂‡πâ‡∏ô‡∏≠‡∏¢‡∏π‡πà‡∏Å‡∏±‡∏ö‡πÄ‡∏û‡∏®‡∏Ç‡∏≠‡∏á‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢ ‡∏´‡πâ‡∏≤‡∏°‡∏ñ‡∏≤‡∏°‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÉ‡∏´‡∏°‡πà‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏≠‡∏∑‡πà‡∏ô

        {dialogue_examples}
        """

        self.message_history = [SystemMessage(content=system_prompt)]
        return system_prompt
    
    def _update_question_status(self, student_input: str):
        """Update question status using GPT-4.1-mini with strict JSON output"""
        if not self.questions_to_ask:
            return
        
        # Only check unasked questions
        unasked_questions = [q for q in self.questions_to_ask if not q["asked"]]
        if not unasked_questions:
            return
        
        # Build numbered question list
        questions_list = "\n".join([f"{i+1}. {q['simulator_ask']}" for i, q in enumerate(unasked_questions)])

        analysis_prompt = [
            {"role": "system", "content": (
                "‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô‡∏ú‡∏π‡πâ‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏û‡∏ó‡∏¢‡πå "
                "‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ß‡πà‡∏≤‡πÅ‡∏û‡∏ó‡∏¢‡πå‡πÑ‡∏î‡πâ‡∏ñ‡∏≤‡∏°**‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô**‡∏Å‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÉ‡∏ô‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà "
                "**‡∏´‡πâ‡∏≤‡∏°**‡∏ï‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏£‡∏¥‡∏°‡∏´‡∏£‡∏∑‡∏≠‡∏ñ‡∏∑‡∏≠‡∏ß‡πà‡∏≤‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡πÇ‡∏î‡∏¢‡∏ô‡∏±‡∏¢\n\n"
                "‡∏Å‡∏é‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏ö‡∏Ñ‡∏π‡πà:\n"
                "- ‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á\n"
                "- ‡∏Å‡∏≤‡∏£‡∏ñ‡∏≤‡∏°‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ (‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡∏°‡∏≤‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÑ‡∏´‡∏£‡πà, ‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡∏≠‡∏∑‡πà‡∏ô, ‡πÄ‡∏Ñ‡∏¢‡∏û‡∏ö‡∏´‡∏°‡∏≠) ‚â† ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á\n"
                "- ‡∏Å‡∏≤‡∏£‡∏ñ‡∏≤‡∏°‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏Å‡∏©‡∏≤ ‚â† ‡∏Å‡∏≤‡∏£‡∏ñ‡∏≤‡∏°‡∏≠‡∏≤‡∏Å‡∏≤‡∏£\n"
                "- ‡∏Å‡∏≤‡∏£‡∏ñ‡∏≤‡∏°‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå‡πÇ‡∏£‡∏Ñ ‚â† ‡∏Å‡∏≤‡∏£‡∏ñ‡∏≤‡∏°‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥\n\n"
                "‡∏ï‡∏≠‡∏ö‡πÄ‡∏õ‡πá‡∏ô JSON array ‡∏Ç‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏•‡∏Ç ‡πÄ‡∏ä‡πà‡∏ô [1, 3] ‡∏´‡∏£‡∏∑‡∏≠ [] ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ"
            )},
            {"role": "user", "content": f"""
                ‡πÅ‡∏û‡∏ó‡∏¢‡πå‡∏û‡∏π‡∏î: "{student_input}"

                ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°:
                {questions_list}

                ‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏•‡∏Ç‡∏Ç‡πâ‡∏≠‡∏ó‡∏µ‡πà‡πÅ‡∏û‡∏ó‡∏¢‡πå‡∏ñ‡∏≤‡∏°**‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô**‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á:"""}]

        try:
            analysis_response = self.client.chat.completions.create(
                messages=analysis_prompt,
                model="gpt-4.1-mini",
                temperature=0.1,
                max_completion_tokens=50
            )
            raw_output = analysis_response.choices[0].message.content.strip()
            
            # Parse strict JSON
            asked_indices = json.loads(raw_output)  # must be list of ints
            for idx in asked_indices:
                if 1 <= idx <= len(unasked_questions):
                    unasked_questions[idx-1]["asked"] = True
                    print(f"üîç Marked as asked: {unasked_questions[idx-1]['simulator_ask'][:50]}...")
        
        except Exception as e:
            print(f"‚ö†Ô∏è Question analysis failed: {e}")

    
    def _keyword_fallback(self, student_input: str):
        """Fallback keyword matching system"""
        for q in self.questions_to_ask:
            if not q["asked"]:
                question_text = q["simulator_ask"]
                # Check if key terms from the question are mentioned in student input
                if any(keyword in student_input for keyword in question_text.split() if len(keyword) > 3):
                    q["asked"] = True
                    print(f"üî§ Keyword fallback: {q['simulator_ask'][:50]}...")
    
    def _build_question_limit_rule(self) -> str:
        """Build question limit rule based on question_limit_type"""
        if not self.questions_to_ask or self.question_limit_type is None:
            return ""  # No rules if no questions
        
        if self.question_limit_type == "multiple":
            return "‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ñ‡∏≤‡∏°‡πÑ‡∏î‡πâ‡∏´‡∏•‡∏≤‡∏¢‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°"
        elif self.question_limit_type == "single":
            return "‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ñ‡∏≤‡∏°‡πÑ‡∏î‡πâ‡πÅ‡∏Ñ‡πà‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏î‡∏µ‡∏¢‡∏ß"
        else:
            return "‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ñ‡∏≤‡∏°‡πÑ‡∏î‡πâ‡πÅ‡∏Ñ‡πà‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏î‡∏µ‡∏¢‡∏ß"  # Default to single
    
    def _build_question_list(self) -> str:
        """Build formatted question list showing only unasked questions"""
        # Handle cases where no questions are available
        if not self.questions_to_ask:
            return "[‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏£‡∏ì‡∏µ‡∏ô‡∏µ‡πâ]"
        
        # Filter only unasked questions
        unasked_questions = [q for q in self.questions_to_ask if not q['asked']]
        
        if not unasked_questions:
            return "[‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏° - ‡∏ú‡∏π‡πâ‡πÄ‡∏Ç‡πâ‡∏≤‡∏™‡∏≠‡∏ö‡πÑ‡∏î‡πâ‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°‡∏ó‡∏∏‡∏Å‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡πÅ‡∏•‡πâ‡∏ß]\n\n**‡∏Å‡∏é‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç**: ‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏≠‡∏ö‡∏ß‡πà‡∏≤ '‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°‡∏Ñ‡πà‡∏∞' ‡∏´‡∏£‡∏∑‡∏≠ '‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°‡∏Ñ‡∏£‡∏±‡∏ö' ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô ‡∏Ç‡∏∂‡πâ‡∏ô‡∏≠‡∏¢‡∏π‡πà‡∏Å‡∏±‡∏ö‡πÄ‡∏û‡∏®‡∏Ç‡∏≠‡∏á‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢ ‡∏´‡πâ‡∏≤‡∏°‡∏ñ‡∏≤‡∏°‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÉ‡∏´‡∏°‡πà"
        
        # Build the question list
        question_list = ""
        for q in unasked_questions:
            question_list += f"- {q['simulator_ask']}\n"
        
        return question_list.strip()
    
    def _refresh_system_prompt(self):
        """Update the system prompt with current question status"""
        # Find the original system message
        for i, msg in enumerate(self.message_history):
            if isinstance(msg, SystemMessage) and "# ‡∏Å‡∏è‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö" in msg.content:
                # This is our main system prompt, update it
                updated_content = self._build_updated_system_content(msg.content)
                self.message_history[i] = SystemMessage(content=updated_content)
                break
    
    def _build_updated_system_content(self, original_content: str) -> str:
        """Build updated system prompt with current question status"""
        # Find the question list section and replace it
        lines = original_content.split('\n')
        updated_lines = []
        skip_until_next_section = False
        
        for line in lines:
            if "‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏° (‡∏ñ‡∏≤‡∏°‡πÄ‡∏â‡∏û‡∏≤‡∏∞ asked=False):" in line:
                # Start of question list section
                updated_lines.append(line)
                updated_lines.append("")
                # Insert current question list
                current_questions = self._build_question_list()
                updated_lines.append(current_questions)
                skip_until_next_section = True
            elif skip_until_next_section and (line.startswith("#") or line.strip().startswith("**‡∏Å‡∏é‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç**") or line.strip() == "‡∏ñ‡πâ‡∏≤‡∏ñ‡∏≤‡∏°‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏´‡∏•‡πà‡∏≤‡∏ô‡∏µ‡πâ‡πÅ‡∏•‡πâ‡∏ß"):
                # End of question list section
                skip_until_next_section = False
                updated_lines.append(line)
            elif not skip_until_next_section:
                updated_lines.append(line)
        
        return '\n'.join(updated_lines)

    def summarize_history(self):
        """Summarize old messages into a structured memory form and sync question flags"""
        if len(self.message_history) <= 6:
            return

        # Collect old conversation (everything except system + last 5 turns)
        old_msgs = self.message_history[1:-5]
        convo_text = "\n".join(
            [f"{'üßë‚Äç‚öïÔ∏è' if isinstance(m, HumanMessage) else 'üë©‚Äç‚öïÔ∏è'}: {m.content}" for m in old_msgs]
        )

        # Build explicit question status list from Python flags
        question_status_list = "\n".join(
            f"- simulator_ask: {q['simulator_ask']}\n  asked: {q.get('asked', False)}"
            for q in self.questions_to_ask
        )

        # Build stricter summarization prompt with injected asked/unasked state
        summarization_prompt = [
            {
                "role": "system",
                "content": (
                    "‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏™‡∏≠‡∏ö OSCE ‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏™‡∏£‡∏∏‡∏õ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≥‡∏Ç‡∏≠‡∏á‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢‡∏à‡∏≥‡∏•‡∏≠‡∏á\n"
                    "‡∏à‡∏á‡∏™‡∏£‡∏∏‡∏õ‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏™‡∏±‡πâ‡∏ô ‡∏Å‡∏£‡∏∞‡∏ä‡∏±‡∏ö ‡πÇ‡∏î‡∏¢‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏∞‡∏ö‡∏∏‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡∏±‡∏á‡∏ô‡∏µ‡πâ:\n"
                    "1. ‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡∏´‡∏£‡∏∑‡∏≠‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏ó‡∏µ‡πà‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢‡πÄ‡∏•‡πà‡∏≤ ‡∏´‡∏£‡∏∑‡∏≠ ‡∏ú‡∏π‡πâ‡∏õ‡∏Å‡∏Ñ‡∏£‡∏≠‡∏á‡∏Ç‡∏≠‡∏á‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢‡πÄ‡∏•‡πà‡∏≤‡πÑ‡∏õ‡πÅ‡∏•‡πâ‡∏ß\n"
                    "2. ‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏ô‡∏±‡∏Å‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡∏´‡∏£‡∏∑‡∏≠‡∏ú‡∏π‡πâ‡∏™‡∏≠‡∏ö‡πÑ‡∏î‡πâ‡∏ñ‡∏≤‡∏°‡πÅ‡∏•‡∏∞‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢ ‡∏´‡∏£‡∏∑‡∏≠ ‡∏ú‡∏π‡πâ‡∏õ‡∏Å‡∏Ñ‡∏£‡∏≠‡∏á‡∏Ç‡∏≠‡∏á‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢‡∏ï‡∏≠‡∏ö‡πÑ‡∏õ‡πÅ‡∏•‡πâ‡∏ß\n"
                    "3. ‡∏≠‡∏≤‡∏£‡∏°‡∏ì‡πå‡∏´‡∏£‡∏∑‡∏≠‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞‡∏ô‡∏¥‡∏™‡∏±‡∏¢‡∏Ç‡∏≠‡∏á‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢ ‡∏´‡∏£‡∏∑‡∏≠ ‡∏ú‡∏π‡πâ‡∏õ‡∏Å‡∏Ñ‡∏£‡∏≠‡∏á‡∏Ç‡∏≠‡∏á‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢‡∏ó‡∏µ‡πà‡∏™‡∏∑‡πà‡∏≠‡∏≠‡∏≠‡∏Å‡∏°‡∏≤\n"
                    "4. ‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏° (‡∏ï‡∏≤‡∏° asked=True/False ‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ‡πÑ‡∏ß‡πâ) "
                    "‡∏≠‡∏¢‡πà‡∏≤‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏Ñ‡πà‡∏≤ ‡∏≠‡∏¢‡πà‡∏≤‡∏Ñ‡∏≤‡∏î‡πÄ‡∏î‡∏≤\n\n"
                    "‡∏Ç‡πâ‡∏≠‡∏´‡πâ‡∏≤‡∏°:\n"
                    "- ‡∏´‡πâ‡∏≤‡∏°‡πÉ‡∏´‡πâ‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏û‡∏ó‡∏¢‡πå\n"
                    "- ‡∏´‡πâ‡∏≤‡∏°‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡∏°‡πà‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏õ‡∏£‡∏≤‡∏Å‡∏è‡πÉ‡∏ô‡∏ö‡∏ó‡∏™‡∏ô‡∏ó‡∏ô‡∏≤\n"
                    "- ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÉ‡∏´‡πâ‡∏≠‡πà‡∏≤‡∏ô‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏á‡πà‡∏≤‡∏¢‡πÅ‡∏•‡∏∞‡∏Å‡∏£‡∏∞‡∏ä‡∏±‡∏ö"
                )
            },
            {
                "role": "user",
                "content": (
                    f"‡∏ô‡∏µ‡πà‡∏Ñ‡∏∑‡∏≠‡∏ö‡∏ó‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô‡∏°‡∏≤:\n{convo_text}\n\n"
                    f"‡∏ô‡∏µ‡πà‡∏Ñ‡∏∑‡∏≠‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏° (‡∏à‡∏≤‡∏Å‡∏£‡∏∞‡∏ö‡∏ö):\n{question_status_list}"
                )
            }
        ]

        try:
            summary_resp = self.client.chat.completions.create(
                messages=summarization_prompt,
                model="gpt-4.1-mini",
                temperature=0.3,
                max_completion_tokens=350
            )
            summary_text = summary_resp.choices[0].message.content.strip()
        except Exception as e:
            summary_text = f"(‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {e})"

        # Replace old history with system prompt + summary + recent turns
        self.message_history = [self.message_history[0]] + [
            SystemMessage(content=f"[‡∏™‡∏£‡∏∏‡∏õ‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤]: {summary_text}")
        ] + self.message_history[-5:]

        # Debug printout
        print("\nüßæ Memory updated with new summary:\n")
        print(f"[SUMMARY] {summary_text}\n")
        print("üìå Current question flags:")
        for q in self.questions_to_ask:
            print(f"   - {q['simulator_ask']} | asked={q['asked']}")



    def manage_memory(self):
        """Decide whether to truncate or summarize"""
        if self.memory_mode == "truncate":
            self.message_history = [self.message_history[0]] + self.message_history[-10:]
        elif self.memory_mode == "summarize":
            self.summarize_history()

    def chat_turn(self, student_message: str) -> Tuple[str, float]:
        """Handle one conversation turn"""
        start_time = time.time()
        try:
            self.message_history.append(HumanMessage(content=student_message))
            
            # ‚úÖ Update question tracking
            self._update_question_status(student_message)
            
            # ‚úÖ Update system prompt with current question status
            self._refresh_system_prompt()

            if len(self.message_history) > self.max_turns_before_memory:
                self.manage_memory()

            messages_payload = []
            for m in self.message_history:
                if isinstance(m, SystemMessage):
                    messages_payload.append({"role": "system", "content": m.content})
                elif isinstance(m, HumanMessage):
                    messages_payload.append({"role": "user", "content": m.content})
                elif isinstance(m, AIMessage):
                    messages_payload.append({"role": "assistant", "content": m.content})

            response = self.client.chat.completions.create(
                messages=messages_payload,
                **self.generation_params
            )

            usage = response.usage
            self.input_tokens += usage.prompt_tokens
            self.output_tokens += usage.completion_tokens
            self.total_tokens += usage.total_tokens

            patient_response = response.choices[0].message.content
            self.message_history.append(AIMessage(content=patient_response))

            return patient_response, time.time() - start_time

        except Exception as e:
            return f"‡∏Ç‡∏≠‡πÇ‡∏ó‡∏©‡∏Ñ‡πà‡∏∞ ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {str(e)}", 0.0

    def show_session_summary(self):
        """Show token usage and stats"""
        print("\nüìä ‡∏™‡∏£‡∏∏‡∏õ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô")
        print(f"üì• Input Tokens: {self.input_tokens}")
        print(f"üì§ Output Tokens: {self.output_tokens}")
        print(f"üìä Total Tokens: {self.total_tokens}")

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--mode", choices=["practice", "exam"], default="practice",
                        help="Choose practice (random variation) or exam (fixed seed) mode")
    parser.add_argument("--model", choices=["gpt-4.1-mini", "gpt-5"], default="gpt-4.1-mini",
                        help="Choose which model to use")
    parser.add_argument("--memory", choices=["none", "truncate", "summarize"], default="summarize",
                        help="Choose memory management strategy")
    args = parser.parse_args()

    case_file = r"C:\Users\ACER\Desktop\testAPI\extracted_data2\02_01_blood_in_stool.json"
    if not os.path.exists(case_file):
        print(f"‚ùå Missing case file: {case_file}")
        sys.exit(1)

    # Choose model here:
    # "gpt-4.1-mini" ‚Üí tunable, interactive
    # "gpt-5"        ‚Üí deterministic, strict
    bot = SimpleChatbotTester(memory_mode="summarize", model_choice="gpt-4.1-mini")

    case_data = bot.load_case_data(case_file)
    bot.setup_conversation(case_data)

    print(f"\nüí¨ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤ (quit = ‡∏à‡∏ö) | Mode: {args.mode}, Model: {args.model}\n")
    while True:
        student = input("üßë‚Äç‚öïÔ∏è ‡∏ô‡∏±‡∏Å‡∏®‡∏∂‡∏Å‡∏©‡∏≤: ").strip()
        if student.lower() in ["quit", "exit", "q"]:
            break
        reply, t = bot.chat_turn(student)
        print(f"üë©‚Äç‚öïÔ∏è ‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢‡∏à‡∏≥‡∏•‡∏≠‡∏á: {reply} (‚è± {t:.2f}s)")

    bot.show_session_summary()

if __name__ == "__main__":
    main()
